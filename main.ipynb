{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 환경 설정\n",
    "\n",
    "1. vscode 내 jupyter notebook 환경에서 input 기능 활용을 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ipywidgets-8.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필수 구현 과제\n",
    "1. 사용 환경 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# 환경 변수에서 호출\n",
    "openai.api_key = os.environ.get(\"MY_OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 모델 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 모델\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\", api_key=openai.api_key\n",
    ")\n",
    "\n",
    "# LLM 모델\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_model = ChatOpenAI(model=\"gpt-4o-mini\", api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 문서 로드하기\n",
    "- 사용 문서\n",
    "  - 초거대 언어모델 연구 동향.pdf\n",
    "  - 박찬준 외, 「초거대 언어모델 연구 동향」, 『정보학회지』, 제41권 제11호(통권 제414호), 한국정보과학회, 2023, 8-24 \n",
    "  - 출처 [¶](https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11610124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "페이지 수: 17\n",
      "\n",
      "[페이지내용]\n",
      "8 특집원고  초거대 언어모델 연구 동향\n",
      "초거대 언어모델 연구 동향\n",
      "업스테이지  박찬준*･이원성･김윤기･김지후･이활석\n",
      " \n",
      "1. 서  론1)\n",
      "ChatGPT1)와 같은 초거대 언어모델(Large Language \n",
      "Model, LLM) 의 등장으로 기존에 병렬적으로 연구되\n",
      "던 다양한 자연언어처리 하위 분야들이 하나의 모델\n",
      "로 처리되고 있으며, 태스크 수렴 현상 (Converge)이 \n",
      "발생하고 있다. 즉 하나의 LLM으로 번역, 요약, 질의\n",
      "응답, 형태소분석 등의 작업을 모두 처리할 수 있게 \n",
      "되었다. 프롬프트 (Prompt)를 어떻게 모델에게 입력하\n",
      "느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이\n",
      "에 따라 사용자의 목적에 맞는 출력을 생성하는 패러\n",
      "다임을 맞이하게 되었다 [1].\n",
      "LLM은 최근 몇 년 간의 연구 동향에 따라 뛰어난 \n",
      "발전을 이루고 있다. 이러한 발전은 몇 가지 주요한 \n",
      "요인에 기반하고 있으며, 이 요인들은 현대 자연언어\n",
      "처리 (Natural Language Proce\n",
      "\n",
      "[metadata]\n",
      "{'source': 'documents/초거대 언어모델 연구 동향.pdf', 'page': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# 가져올 pdf 파일 경로\n",
    "path = \"documents/초거대 언어모델 연구 동향.pdf\"\n",
    "\n",
    "# 사용할 pdf loader 선택\n",
    "loader = PyPDFLoader(path)\n",
    "\n",
    "# pdf 파일 불러오기\n",
    "docs = loader.load()\n",
    "\n",
    "# 불러올 범위 설정\n",
    "page = 0\n",
    "start_point = 0\n",
    "end_point = 500\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"페이지 수: {len(docs)}\")\n",
    "print(f\"\\n[페이지내용]\\n{docs[page].page_content[start_point:end_point]}\")\n",
    "print(f\"\\n[metadata]\\n{docs[page].metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 문서 청크로 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정\n",
    "CHUNK_INDEX = 0\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "SEPERATOR = \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i .`CharacterTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_splits\n",
      "길이 : 142\n",
      " 결과 확인 : 8 특집원고  초거대 언어모델 연구 동향\n",
      "초거대 언어모델 연구 동향\n",
      "업스테이지  박찬준*･이원성･김윤기･김지후･이활석\n",
      " \n",
      "1. 서  론1)\n",
      "ChatGPT1)와 같은 초거대 언어모델(Large Language \n",
      "Model, LLM) 의 등장으로 기존에 병렬적으로 연구되\n",
      "던 다양한 자연언어처리 하위 분야들이 하나의 모델\n",
      "로 처리되고 있으며, 태스크 수렴 현상 (Converge)이 \n",
      "발생하고 있다. 즉 하나의 LLM으로 번역, 요약, 질의\n",
      "응답, 형태소분석 등의 작업을 모두 처리할 수 있게 \n",
      "되었다. 프롬프트 (Prompt)를 어떻게 모델에게 입력하\n",
      "느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이\n",
      "에 따라 사용자의 목적에 맞는 출력을 생성하는 패러\n",
      "다임을 맞이하게 되었다 [1].\n",
      "LLM은 최근 몇 년 간의 연구 동향에 따라 뛰어난 \n",
      "발전을 이루고 있다. 이러한 발전은 몇 가지 주요한 \n",
      "요인에 기반하고 있으며, 이 요인들은 현대 자연언어\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 문서 분할기 설정\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=SEPERATOR,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    is_separator_regex=True,\n",
    ")\n",
    "\n",
    "# 문서 분할\n",
    "c_splits = splitter.split_documents(docs)\n",
    "\n",
    "# 결과 확인\n",
    "print(\n",
    "    f\"c_splits\\n길이 : {len(c_splits)}\\n 결과 확인 : {c_splits[CHUNK_INDEX].page_content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii .`RecursiveCharacterTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_splits\n",
      "길이 : 141\n",
      " 결과 확인 : 8 특집원고  초거대 언어모델 연구 동향\n",
      "초거대 언어모델 연구 동향\n",
      "업스테이지  박찬준*･이원성･김윤기･김지후･이활석\n",
      " \n",
      "1. 서  론1)\n",
      "ChatGPT1)와 같은 초거대 언어모델(Large Language \n",
      "Model, LLM) 의 등장으로 기존에 병렬적으로 연구되\n",
      "던 다양한 자연언어처리 하위 분야들이 하나의 모델\n",
      "로 처리되고 있으며, 태스크 수렴 현상 (Converge)이 \n",
      "발생하고 있다. 즉 하나의 LLM으로 번역, 요약, 질의\n",
      "응답, 형태소분석 등의 작업을 모두 처리할 수 있게 \n",
      "되었다. 프롬프트 (Prompt)를 어떻게 모델에게 입력하\n",
      "느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이\n",
      "에 따라 사용자의 목적에 맞는 출력을 생성하는 패러\n",
      "다임을 맞이하게 되었다 [1].\n",
      "LLM은 최근 몇 년 간의 연구 동향에 따라 뛰어난 \n",
      "발전을 이루고 있다. 이러한 발전은 몇 가지 주요한 \n",
      "요인에 기반하고 있으며, 이 요인들은 현대 자연언어\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 문서 분할기 설정\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# 문서 분할\n",
    "r_splits = recursive_splitter.split_documents(docs)\n",
    "\n",
    "# 결과 확인\n",
    "print(\n",
    "    f\"r_splits\\n길이 : {len(r_splits)}\\n 결과 확인 : {r_splits[CHUNK_INDEX].page_content}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 생성중...\n",
      "Sample_1 0 \n",
      " 8 특집원고  초거대 언어모델 연구 동향\n",
      "초거대 언어모델 연구 동향\n",
      "업스테이지  박찬준*･이원성･김윤기･김지후･이활석\n",
      " \n",
      "1. 서  론1)\n",
      "ChatGPT1)와 같은 초거대 언어모델(Large Language \n",
      "Model, LLM) 의 등장으로 기존에 병렬적으로 연구되\n",
      "던 다양한 자연언어처리 하위 분야들이 하나의 모델\n",
      "로 처리되고 있으며, 태스크 수렴 현상 (Converge)이 \n",
      "발생하고 있다. 즉 하나의 LLM으로 번역, 요약, 질의\n",
      "응답, 형태소분석 등의 작업을 모두 처리할 수 있게 \n",
      "되었다. 프롬프트 (Prompt)를 어떻게 모델에게 입력하\n",
      "느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이\n",
      "에 따라 사용자의 목적에 맞는 출력을 생성하는 패러\n",
      "다임을 맞이하게 되었다 [1].\n",
      "LLM은 최근 몇 년 간의 연구 동향에 따라 뛰어난 \n",
      "발전을 이루고 있다. 이러한 발전은 몇 가지 주요한 \n",
      "요인에 기반하고 있으며, 이 요인들은 현대 자연언어\n",
      "Sample_2 0 \n",
      " 8 특집원고  초거대 언어모델 연구 동향\n",
      "초거대 언어모델 연구 동향\n",
      "업스테이지  박찬준*･이원성･김윤기･김지후･이활석\n",
      " \n",
      "1. 서  론1)\n",
      "ChatGPT1)와 같은 초거대 언어모델(Large Language \n",
      "Model, LLM) 의 등장으로 기존에 병렬적으로 연구되\n",
      "던 다양한 자연언어처리 하위 분야들이 하나의 모델\n",
      "로 처리되고 있으며, 태스크 수렴 현상 (Converge)이 \n",
      "발생하고 있다. 즉 하나의 LLM으로 번역, 요약, 질의\n",
      "응답, 형태소분석 등의 작업을 모두 처리할 수 있게 \n",
      "되었다. 프롬프트 (Prompt)를 어떻게 모델에게 입력하\n",
      "느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이\n",
      "에 따라 사용자의 목적에 맞는 출력을 생성하는 패러\n",
      "다임을 맞이하게 되었다 [1].\n",
      "LLM은 최근 몇 년 간의 연구 동향에 따라 뛰어난 \n",
      "발전을 이루고 있다. 이러한 발전은 몇 가지 주요한 \n",
      "요인에 기반하고 있으며, 이 요인들은 현대 자연언어\n",
      "Sample 생성중...\n",
      "Sample_1 1 \n",
      " LLM은 최근 몇 년 간의 연구 동향에 따라 뛰어난 \n",
      "발전을 이루고 있다. 이러한 발전은 몇 가지 주요한 \n",
      "요인에 기반하고 있으며, 이 요인들은 현대 자연언어\n",
      "처리 (Natural Language Processing, NLP) 연구의 핵심\n",
      "적인 추세로 간주된다. 첫째로, 데이터의 양적 확대는 \n",
      "무시할 수 없는 중요한 요인이다. 디지털화의 선도로, \n",
      "텍스트 데이터의 양이 기하급수적으로 증가하였고, \n",
      "이는 연구의 질적 변화를 가져왔다. 대규모 코퍼스의 \n",
      "활용은 LLM의 일반화 능력을 향상시키며, 다양한 맥\n",
      "락과 주제에 대한 깊은 학습을 가능하게 한다. 둘째\n",
      "로, 컴퓨팅 기술의 진보는 LLM의 발전에 있어 결정\n",
      "적이었다. 특히, Graphics Processing Unit (GPU) 및 \n",
      "Tensor Processing Unit (TPU) 와 같은 고성능 병렬 처\n",
      "리 하드웨어의 개발은 모델 학습에 있어 병목 현상을 \n",
      "크게 완화시켰다. 이로 인해 연구자들은 모델의 복잡\n",
      "Sample_2 1 \n",
      " LLM은 최근 몇 년 간의 연구 동향에 따라 뛰어난 \n",
      "발전을 이루고 있다. 이러한 발전은 몇 가지 주요한 \n",
      "요인에 기반하고 있으며, 이 요인들은 현대 자연언어\n",
      "처리 (Natural Language Processing, NLP) 연구의 핵심\n",
      "적인 추세로 간주된다. 첫째로, 데이터의 양적 확대는 \n",
      "무시할 수 없는 중요한 요인이다. 디지털화의 선도로, \n",
      "텍스트 데이터의 양이 기하급수적으로 증가하였고, \n",
      "이는 연구의 질적 변화를 가져왔다. 대규모 코퍼스의 \n",
      "활용은 LLM의 일반화 능력을 향상시키며, 다양한 맥\n",
      "락과 주제에 대한 깊은 학습을 가능하게 한다. 둘째\n",
      "로, 컴퓨팅 기술의 진보는 LLM의 발전에 있어 결정\n",
      "적이었다. 특히, Graphics Processing Unit (GPU) 및 \n",
      "Tensor Processing Unit (TPU) 와 같은 고성능 병렬 처\n",
      "리 하드웨어의 개발은 모델 학습에 있어 병목 현상을 \n",
      "크게 완화시켰다. 이로 인해 연구자들은 모델의 복잡\n",
      "Sample 생성중...\n",
      "Sample_1 2 \n",
      " 리 하드웨어의 개발은 모델 학습에 있어 병목 현상을 \n",
      "크게 완화시켰다. 이로 인해 연구자들은 모델의 복잡\n",
      "성을 키우고, 더욱 깊은 신경망 구조를 탐구할 수 있\n",
      "게 되었다. 셋째, 알고리즘 및 기술의 발전은 LLM의 \n",
      "성능 향상을 주도하였다. Attention 및 Transformer \n",
      "Architecture의 도입은 연구자들에게 문맥 간의 관계\n",
      "를 더욱 정교하게 모델링할 수 있는 방법을 제공하였\n",
      "다 [2, 3]. 이 모든 변화의 중심에는 ‘scaling law’라는 \n",
      "* 정회원\n",
      "1) https://openai.com/blog/chatgpt\n",
      "학문적인 통찰이 있다 [4]. 해당 연구에 따르면, 모델\n",
      "의 크기와 그 성능은 긍정적인 상관 관계를 보인다. \n",
      "이를 통해 연구자들은 모델의 파라미터 수를 증가시\n",
      "키면서, 이에 따른 성능 향상을 기술적 진보의 상호 \n",
      "작용에서 나온 결과이며, 이러한 추세는 앞으로도 \n",
      "NLP 연구의 주요 동력이 될 것으로 예상된다.\n",
      "Sample_2 2 \n",
      " 리 하드웨어의 개발은 모델 학습에 있어 병목 현상을 \n",
      "크게 완화시켰다. 이로 인해 연구자들은 모델의 복잡\n",
      "성을 키우고, 더욱 깊은 신경망 구조를 탐구할 수 있\n",
      "게 되었다. 셋째, 알고리즘 및 기술의 발전은 LLM의 \n",
      "성능 향상을 주도하였다. Attention 및 Transformer \n",
      "Architecture의 도입은 연구자들에게 문맥 간의 관계\n",
      "를 더욱 정교하게 모델링할 수 있는 방법을 제공하였\n",
      "다 [2, 3]. 이 모든 변화의 중심에는 ‘scaling law’라는 \n",
      "* 정회원\n",
      "1) https://openai.com/blog/chatgpt\n",
      "학문적인 통찰이 있다 [4]. 해당 연구에 따르면, 모델\n",
      "의 크기와 그 성능은 긍정적인 상관 관계를 보인다. \n",
      "이를 통해 연구자들은 모델의 파라미터 수를 증가시\n",
      "키면서, 이에 따른 성능 향상을 기술적 진보의 상호 \n",
      "작용에서 나온 결과이며, 이러한 추세는 앞으로도 \n",
      "NLP 연구의 주요 동력이 될 것으로 예상된다.\n",
      "Sample 생성중...\n",
      "Sample_1 3 \n",
      " 키면서, 이에 따른 성능 향상을 기술적 진보의 상호 \n",
      "작용에서 나온 결과이며, 이러한 추세는 앞으로도 \n",
      "NLP 연구의 주요 동력이 될 것으로 예상된다.\n",
      "연구단계를 넘어 LLM은 산업계에서도 많은 발전\n",
      "을 이루어 내고 있다. LLM 은 교육, 의료, 금융, 제조 \n",
      "등 거의 모든 산업 분야에서 광범위한 활용 가능성을 \n",
      "제시하고 있다 [5, 6, 7, 8]. 교육 분야에서는 단순한 \n",
      "정보 검색을 넘어, 개인화된 학습 경로를 추천하는 시\n",
      "스템, 과제의 자동 평가, 학생들의 복잡한 질문에 대\n",
      "한 답변 제공 등의 역할로 활용될 수 있다. 이는 교육\n",
      "의 효율성과 개인화를 동시에 추구하는 현대의 교육 \n",
      "트렌드와 맞물려 큰 효과를 발휘할 것으로 기대된다. \n",
      "의료 분야에서는 환자 데이터를 기반으로 한 초기 진\n",
      "단 도구로 활용될 뿐만 아니라, 복잡한 의료 기록 분\n",
      "석, 신약 개발에 필요한 연구 데이터 분석, 또는 최신 \n",
      "의학 연구 동향 파악 등의 다양한 역할을 수행할 수\n",
      "Sample_2 3 \n",
      " 키면서, 이에 따른 성능 향상을 기술적 진보의 상호 \n",
      "작용에서 나온 결과이며, 이러한 추세는 앞으로도 \n",
      "NLP 연구의 주요 동력이 될 것으로 예상된다.\n",
      "연구단계를 넘어 LLM은 산업계에서도 많은 발전\n",
      "을 이루어 내고 있다. LLM 은 교육, 의료, 금융, 제조 \n",
      "등 거의 모든 산업 분야에서 광범위한 활용 가능성을 \n",
      "제시하고 있다 [5, 6, 7, 8]. 교육 분야에서는 단순한 \n",
      "정보 검색을 넘어, 개인화된 학습 경로를 추천하는 시\n",
      "스템, 과제의 자동 평가, 학생들의 복잡한 질문에 대\n",
      "한 답변 제공 등의 역할로 활용될 수 있다. 이는 교육\n",
      "의 효율성과 개인화를 동시에 추구하는 현대의 교육 \n",
      "트렌드와 맞물려 큰 효과를 발휘할 것으로 기대된다. \n",
      "의료 분야에서는 환자 데이터를 기반으로 한 초기 진\n",
      "단 도구로 활용될 뿐만 아니라, 복잡한 의료 기록 분\n",
      "석, 신약 개발에 필요한 연구 데이터 분석, 또는 최신 \n",
      "의학 연구 동향 파악 등의 다양한 역할을 수행할 수\n",
      "Sample 생성중...\n",
      "Sample_1 4 \n",
      " 단 도구로 활용될 뿐만 아니라, 복잡한 의료 기록 분\n",
      "석, 신약 개발에 필요한 연구 데이터 분석, 또는 최신 \n",
      "의학 연구 동향 파악 등의 다양한 역할을 수행할 수 \n",
      "있다. 이로써 의료 전문가들의 결정을 보조하고, 효율\n",
      "적인 치료 방향을 도모할 수 있게 된다. 금융 분야에\n",
      "서는 개인의 투자 성향과 시장의 동향을 분석하여 투\n",
      "자 권고를 제공하는 것 외에도, 금융 위험을 상세하게 \n",
      "분석하거나, 복잡한 금융 거래를 자동화하는 시스템\n",
      "의 핵심 구성 요소로서의 역할을 할 수 있다. 이는 금\n",
      "융 서비스의 효율과 안전성 향상에 크게 기여할 것이\n",
      "다. 제조 분야에서도 LLM은 설계 단계부터 생산, 품\n",
      "질 관리에 이르기까지의 전 과정에서 데이터 분석 및 \n",
      "최적화 도구로 활용될 수 있다. 생산 효율성 향상과 \n",
      "제품 품질 향상을 도모하며, 고객의 니즈에 더욱 민첩\n",
      "하게 대응할 수 있는 기회를 제공한다.\n",
      "그러나, 이러한 긍정적인 측면들과 더불어 LLM의\n",
      "Sample_2 4 \n",
      " 단 도구로 활용될 뿐만 아니라, 복잡한 의료 기록 분\n",
      "석, 신약 개발에 필요한 연구 데이터 분석, 또는 최신 \n",
      "의학 연구 동향 파악 등의 다양한 역할을 수행할 수 \n",
      "있다. 이로써 의료 전문가들의 결정을 보조하고, 효율\n",
      "적인 치료 방향을 도모할 수 있게 된다. 금융 분야에\n",
      "서는 개인의 투자 성향과 시장의 동향을 분석하여 투\n",
      "자 권고를 제공하는 것 외에도, 금융 위험을 상세하게 \n",
      "분석하거나, 복잡한 금융 거래를 자동화하는 시스템\n",
      "의 핵심 구성 요소로서의 역할을 할 수 있다. 이는 금\n",
      "융 서비스의 효율과 안전성 향상에 크게 기여할 것이\n",
      "다. 제조 분야에서도 LLM은 설계 단계부터 생산, 품\n",
      "질 관리에 이르기까지의 전 과정에서 데이터 분석 및 \n",
      "최적화 도구로 활용될 수 있다. 생산 효율성 향상과 \n",
      "제품 품질 향상을 도모하며, 고객의 니즈에 더욱 민첩\n",
      "하게 대응할 수 있는 기회를 제공한다.\n",
      "그러나, 이러한 긍정적인 측면들과 더불어 LLM의\n",
      "Sample 생성중...\n",
      "Sample_1 5 \n",
      " 제품 품질 향상을 도모하며, 고객의 니즈에 더욱 민첩\n",
      "하게 대응할 수 있는 기회를 제공한다.\n",
      "그러나, 이러한 긍정적인 측면들과 더불어 LLM의 \n",
      "한계점과 위험성도 고려되어야 한다. LLM 은 학습 데\n",
      "이터의 편향성을 그대로 반영할 수 있어, 편향된 결과\n",
      "나 추천을 할 가능성이 있다 [9]. 이는 특히 중요한 의\n",
      "특집원고\n",
      "Sample_2 5 \n",
      " 제품 품질 향상을 도모하며, 고객의 니즈에 더욱 민첩\n",
      "하게 대응할 수 있는 기회를 제공한다.\n",
      "그러나, 이러한 긍정적인 측면들과 더불어 LLM의 \n",
      "한계점과 위험성도 고려되어야 한다. LLM 은 학습 데\n",
      "이터의 편향성을 그대로 반영할 수 있어, 편향된 결과\n",
      "나 추천을 할 가능성이 있다 [9]. 이는 특히 중요한 의\n",
      "특집원고\n",
      "Sample 생성중...\n",
      "Sample_1 6 \n",
      " 2023. 11 정보과학회지 9\n",
      "사 결정을 위해 LLM을 활용하는 경우에 문제가 될 \n",
      "수 있다. 또한, LLM 을 악의적인 목적으로 사용하는 \n",
      "위험성도 있다 [10]. 예를 들면, 미스리딩 정보 생성이 \n",
      "나 편향된 정보 전파를 위한 도구로 활용될 수 있다. \n",
      "이 외에도 LLM의 동작 원리나 결과에 대한 설명력 \n",
      "부족, 최신 정보를 반영하는 데의 한계 등 여러 문제\n",
      "점이 있으며, 이러한 문제점들을 해결하는 것은 다가\n",
      "오는 연구의 중요한 도전 과제로 여겨진다.\n",
      "즉 편향성 (LLM은 학습 데이터에 포함된 편향을 \n",
      "반영할 수 있음), 안전성 (LLM을 악의적인 목적으로 \n",
      "사용할 수 있음), 설명 가능성 (LLM의 예측 결과를 \n",
      "설명하기 어려움), 최신성 (최신정보를 반영하기 어려\n",
      "움)의 문제점을 여전히 LLM의 한계점으로 보유하고 \n",
      "있으며 이러한 문제는 장기적으로 해결하기 위해 연\n",
      "구되어야할 것이다.\n",
      "본 논문은 초거대 언어모델(LLM)에 대한 전반적인\n",
      "Sample_2 6 \n",
      " 2023. 11 정보과학회지 9\n",
      "사 결정을 위해 LLM을 활용하는 경우에 문제가 될 \n",
      "수 있다. 또한, LLM 을 악의적인 목적으로 사용하는 \n",
      "위험성도 있다 [10]. 예를 들면, 미스리딩 정보 생성이 \n",
      "나 편향된 정보 전파를 위한 도구로 활용될 수 있다. \n",
      "이 외에도 LLM의 동작 원리나 결과에 대한 설명력 \n",
      "부족, 최신 정보를 반영하는 데의 한계 등 여러 문제\n",
      "점이 있으며, 이러한 문제점들을 해결하는 것은 다가\n",
      "오는 연구의 중요한 도전 과제로 여겨진다.\n",
      "즉 편향성 (LLM은 학습 데이터에 포함된 편향을 \n",
      "반영할 수 있음), 안전성 (LLM을 악의적인 목적으로 \n",
      "사용할 수 있음), 설명 가능성 (LLM의 예측 결과를 \n",
      "설명하기 어려움), 최신성 (최신정보를 반영하기 어려\n",
      "움)의 문제점을 여전히 LLM의 한계점으로 보유하고 \n",
      "있으며 이러한 문제는 장기적으로 해결하기 위해 연\n",
      "구되어야할 것이다.\n",
      "본 논문은 초거대 언어모델(LLM)에 대한 전반적인\n",
      "Sample 생성중...\n",
      "Sample_1 7 \n",
      " 움)의 문제점을 여전히 LLM의 한계점으로 보유하고 \n",
      "있으며 이러한 문제는 장기적으로 해결하기 위해 연\n",
      "구되어야할 것이다.\n",
      "본 논문은 초거대 언어모델(LLM)에 대한 전반적인 \n",
      "동향을 다루고자 작성되었다. 첫째로 초기의 언어모\n",
      "델부터 현재의 초거대 언어모델까지의 연구 및 발전 \n",
      "과정을 소개한다. 둘째로, 한국어 초거대 언어모델의 \n",
      "특징 및 최근 동향을 조명한다. 셋째로, 최신 초거대 \n",
      "언어모델 연구 동향을 심층적으로 살펴본다. 넷째로, \n",
      "초거대 언어모델의 성능 평가 방식과 그 변화에 대해 \n",
      "논의한다. 마지막으로, 초거대 언어모델 연구와 활용\n",
      "에 있어 중요하게 여겨지는 윤리적 원칙과 관련된 \n",
      "최근의 동향을 소개 한다. 본 논문을 통해 초거대 언\n",
      "어모델에 관한 전반적인 동향과 중요한 주제들에 대\n",
      "한 체계적인 이해를 제공하고, 이 분야의 연구자 및 \n",
      "관련 전문가들에게 유용한 통찰과 지침을 제시하고자 \n",
      "한다.\n",
      "2. 언어모델부터 초거대언어모델까지\n",
      "Sample_2 7 \n",
      " 움)의 문제점을 여전히 LLM의 한계점으로 보유하고 \n",
      "있으며 이러한 문제는 장기적으로 해결하기 위해 연\n",
      "구되어야할 것이다.\n",
      "본 논문은 초거대 언어모델(LLM)에 대한 전반적인 \n",
      "동향을 다루고자 작성되었다. 첫째로 초기의 언어모\n",
      "델부터 현재의 초거대 언어모델까지의 연구 및 발전 \n",
      "과정을 소개한다. 둘째로, 한국어 초거대 언어모델의 \n",
      "특징 및 최근 동향을 조명한다. 셋째로, 최신 초거대 \n",
      "언어모델 연구 동향을 심층적으로 살펴본다. 넷째로, \n",
      "초거대 언어모델의 성능 평가 방식과 그 변화에 대해 \n",
      "논의한다. 마지막으로, 초거대 언어모델 연구와 활용\n",
      "에 있어 중요하게 여겨지는 윤리적 원칙과 관련된 \n",
      "최근의 동향을 소개 한다. 본 논문을 통해 초거대 언\n",
      "어모델에 관한 전반적인 동향과 중요한 주제들에 대\n",
      "한 체계적인 이해를 제공하고, 이 분야의 연구자 및 \n",
      "관련 전문가들에게 유용한 통찰과 지침을 제시하고자 \n",
      "한다.\n",
      "2. 언어모델부터 초거대언어모델까지\n",
      "Sample 생성중...\n",
      "Sample_1 8 \n",
      " 한 체계적인 이해를 제공하고, 이 분야의 연구자 및 \n",
      "관련 전문가들에게 유용한 통찰과 지침을 제시하고자 \n",
      "한다.\n",
      "2. 언어모델부터 초거대언어모델까지\n",
      "자연언어란 “인간의 언어”를 의미하며, 자연언어처\n",
      "리는 자연언어를 컴퓨터가 처리하는 것을 의미한다. \n",
      "자연언어처리를 위해서는 인간의 언어표현 체계를 컴\n",
      "퓨터가 이해할 수 있는 형태로 변환해주는 것이 필요\n",
      "하다. 이러한 역할을 하는 것이 바로 언어모델이다. \n",
      "이번 섹션에서는 전통적인 언어모델 연구에 대해 먼\n",
      "저 살펴보고, 의미기반 언어모델 연구, 문맥기반 언어\n",
      "모델 연구, 초거대 언어모델 연구들에 대해 차례로 살\n",
      "펴본다.\n",
      "전통적인 언어모델 연구 전통적인 언어모델은 인간\n",
      "이 사용하는 단어를 컴퓨터가 이해할 수 있는 숫자 \n",
      "체계로 변환하는 데에 초점을 맞춰 발전했다. 이를 위\n",
      "해 전통적인 언어모델은 단어 집합 (vocabulary)을 생\n",
      "성하고, 단어 집합을 이용하여 자연언어를 컴퓨터가 \n",
      "이해할 수 있는 형태로 변환했다. 단어 집합을 이용하\n",
      "Sample_2 8 \n",
      " 한 체계적인 이해를 제공하고, 이 분야의 연구자 및 \n",
      "관련 전문가들에게 유용한 통찰과 지침을 제시하고자 \n",
      "한다.\n",
      "2. 언어모델부터 초거대언어모델까지\n",
      "자연언어란 “인간의 언어”를 의미하며, 자연언어처\n",
      "리는 자연언어를 컴퓨터가 처리하는 것을 의미한다. \n",
      "자연언어처리를 위해서는 인간의 언어표현 체계를 컴\n",
      "퓨터가 이해할 수 있는 형태로 변환해주는 것이 필요\n",
      "하다. 이러한 역할을 하는 것이 바로 언어모델이다. \n",
      "이번 섹션에서는 전통적인 언어모델 연구에 대해 먼\n",
      "저 살펴보고, 의미기반 언어모델 연구, 문맥기반 언어\n",
      "모델 연구, 초거대 언어모델 연구들에 대해 차례로 살\n",
      "펴본다.\n",
      "전통적인 언어모델 연구 전통적인 언어모델은 인간\n",
      "이 사용하는 단어를 컴퓨터가 이해할 수 있는 숫자 \n",
      "체계로 변환하는 데에 초점을 맞춰 발전했다. 이를 위\n",
      "해 전통적인 언어모델은 단어 집합 (vocabulary)을 생\n",
      "성하고, 단어 집합을 이용하여 자연언어를 컴퓨터가 \n",
      "이해할 수 있는 형태로 변환했다. 단어 집합을 이용하\n",
      "Sample 생성중...\n",
      "Sample_1 9 \n",
      " 해 전통적인 언어모델은 단어 집합 (vocabulary)을 생\n",
      "성하고, 단어 집합을 이용하여 자연언어를 컴퓨터가 \n",
      "이해할 수 있는 형태로 변환했다. 단어 집합을 이용하\n",
      "여 자연언어를 표현하는 전통적인 방법 중 대표적으\n",
      "로 사용되는 방법은 원-핫 인코딩 (one-hot encoding)\n",
      "이다. 원-핫 인코딩은 표현하고자 하는 단어의 색인 \n",
      "(index)에만 1을 표시 하고, 다른 단어의 색인에는 0을 \n",
      "표시하여 단어를 벡터로 나타내는 방법이다. 따라서, \n",
      "원-핫 인코딩을 사용하면 모든 단어를 단어 집합의 \n",
      "크기를 가지는 벡터로 표현할 수 있다. 이러한 벡터를 \n",
      "희소 벡터 (sparse vector)라고 부른다. 예를 들어, 단\n",
      "어 집합에 ‘강아지’, ‘ 고양이’라는 2개의 단어만 존재\n",
      "한다고 했을 때, 원-핫 인코딩 방식으로 이를 표현하\n",
      "면 ‘강아지’는 [1, 0], ‘ 고양이’는 [0, 1] 로 표현된다.\n",
      "그러나, 원-핫 인코딩은 단어 간의 의미적인 연관성\n",
      "Sample_2 9 \n",
      " 해 전통적인 언어모델은 단어 집합 (vocabulary)을 생\n",
      "성하고, 단어 집합을 이용하여 자연언어를 컴퓨터가 \n",
      "이해할 수 있는 형태로 변환했다. 단어 집합을 이용하\n",
      "여 자연언어를 표현하는 전통적인 방법 중 대표적으\n",
      "로 사용되는 방법은 원-핫 인코딩 (one-hot encoding)\n",
      "이다. 원-핫 인코딩은 표현하고자 하는 단어의 색인 \n",
      "(index)에만 1을 표시 하고, 다른 단어의 색인에는 0을 \n",
      "표시하여 단어를 벡터로 나타내는 방법이다. 따라서, \n",
      "원-핫 인코딩을 사용하면 모든 단어를 단어 집합의 \n",
      "크기를 가지는 벡터로 표현할 수 있다. 이러한 벡터를 \n",
      "희소 벡터 (sparse vector)라고 부른다. 예를 들어, 단\n",
      "어 집합에 ‘강아지’, ‘ 고양이’라는 2개의 단어만 존재\n",
      "한다고 했을 때, 원-핫 인코딩 방식으로 이를 표현하\n",
      "면 ‘강아지’는 [1, 0], ‘ 고양이’는 [0, 1] 로 표현된다.\n",
      "그러나, 원-핫 인코딩은 단어 간의 의미적인 연관성\n"
     ]
    }
   ],
   "source": [
    "# Sample 10개 테스트\n",
    "\n",
    "def sample_printer(splits_1, splits_2, n):\n",
    "    for i in range(n):\n",
    "        print(f\"Sample 생성중...\")\n",
    "        print(f\"Sample_1 {i} \\n {splits_1[i].page_content}\")\n",
    "        print(f\"Sample_2 {i} \\n {splits_2[i].page_content}\")\n",
    "\n",
    "sample_printer(c_splits, r_splits, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 벡터 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\", api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 벡터 스토어 생성\n",
    "- `RecursiveCharacterTextSplitter` 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RecursiveCharacterTextSplitter 결과 사용\n",
    "splits = r_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.345] 16 특집원고  초거대 언어모델 연구 동향\n",
      "Retrieval Augmented Generation (RAG) [95, 96, 97, \n",
      "98]이라 한다.\n",
      "Other Tools L a M D A  [ 4 4 ]는 대화 애플리케이션에 \n",
      "특화된 LLM으로, 검색 모듈, 계산기 및 번역기 등의 \n",
      "외부 도구 호출 기능을 가지고 있다. WebGPT [99] 는 \n",
      "웹 브라우저와의 상호작용을 통해 검색 쿼리에 사실 \n",
      "기반의 답변과 함께 출처 정보를 제공 한다. PAL \n",
      "[100]은 Python 인터프리터를 통한 복잡한 기호 추론 \n",
      "기능을 제공하며, 여러 관련 벤치마크에서 뛰어난 성\n",
      "능을 보여주었다. 다양한 종류의 API (e.g., 계산기, 달\n",
      "력, 검색, QA, 번역 등 단순한 API에서부터 Torch/ \n",
      "TensorFlow/HuggingFace Hub에 이르는 복잡한 API까\n",
      "지) 호출 기능을 갖춘 연구들 [101, 102, 103, 104, [{'source': 'documents/초거대 언어모델 연구 동향.pdf', 'page': 8}]\n",
      "* [SIM=0.424] 같은 문자라면 같은 밀집 벡터로 표현하기 때문에 문\n",
      "맥 정보를 반영하지 못한다는 한계를 지닌다.\n",
      "문맥기반 언어모델 연구 문맥 정보를 반영하여 언\n",
      "어를 표현하기 위해, 텍스트 내의 정보를 이용하는 \n",
      "RNN (Recurrent Neural Network) 이 등장했다. 그러나, \n",
      "RNN은 입력 텍스트의 길이가 길어질수록 앞쪽에 위 [{'source': 'documents/초거대 언어모델 연구 동향.pdf', 'page': 1}]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "index = faiss.IndexFlatL2(len(embedding_model.embed_query(\"This is Sample Text.\")))\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embedding_model,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(splits))]\n",
    "vector_store.add_documents(documents=splits, ids=uuids)\n",
    "\n",
    "# 테스트\n",
    "results_with_scores = vector_store.similarity_search_with_score(\n",
    "    \"RAG에 대해 이야기해주세요.\", k=2, filter={\"source\": 'documents/초거대 언어모델 연구 동향.pdf'}\n",
    ")\n",
    "for res, score in results_with_scores:\n",
    "    print(f\"* [SIM={score:.3f}] {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. FAISS를 Retriever로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: documents/초거대 언어모델 연구 동향.pdf | Page: 8\n",
      "Content: 16 특집원고  초거대 언어모델 연구 동향 Retrieval Augmented Generation (RAG) [95, 96, 97,  98]이라 한다. Other Tools L a M D A  [ 4 4 ]는 대화 애플리케이션에  특화된 LLM으로, 검색 모듈, 계산기 및 번역기 등의  외부 도구 호출 기능을 가지고 있다. WebGPT [99] 는  웹 브라우저와의 상호작용을 통해 검색 쿼리에 사실  기반의 답변과 함께 출처 정보를 제공 한다. PAL  [100]은 Python 인터프리터를 통한 복잡한 기호 추론  기능을 제공하며, 여러 관련 벤치마크에서 뛰어난 성 능을 보여주었다. 다양한 종류의 API (e.g., 계산기, 달 력, 검색, QA, 번역 등 단순한 API에서부터 Torch/  TensorFlow/HuggingFace Hub에 이르는 복잡한 API까 지) 호출 기능을 갖춘 연구들 [101, 102, 103, 104,\n",
      "\n",
      "Source: documents/초거대 언어모델 연구 동향.pdf | Page: 1\n",
      "Content: 같은 문자라면 같은 밀집 벡터로 표현하기 때문에 문 맥 정보를 반영하지 못한다는 한계를 지닌다. 문맥기반 언어모델 연구 문맥 정보를 반영하여 언 어를 표현하기 위해, 텍스트 내의 정보를 이용하는  RNN (Recurrent Neural Network) 이 등장했다. 그러나,  RNN은 입력 텍스트의 길이가 길어질수록 앞쪽에 위\n",
      "\n",
      "Source: documents/초거대 언어모델 연구 동향.pdf | Page: 2\n",
      "Content: 계를 그대로 가진다: 1) 하나의 벡터에 텍스트의 모든  정보를 담기 때문에 정보 손실이 발생하고, 2) 입력  텍스트의 길이가 길어지면 기울기 소실 (gradient  vanishing)이 발생한다. 이러한 한계를 해결하기 위해 나온 것이 바로  Attention Mechanism [2] 과 이를 활용한 Transformer  Architecture [3] 이다. Attention Mechanism 은 하나의  벡터에 텍스트의 모든 정보를 담는 RNN, LSTM,  GRU와 다르게, 텍스트 내 단어들의 벡터들을 필요에  따라 적절히 활용하는 메커니즘이다. 현재 언어모델 의 근간이 되는 Transformer가 바로 이러한 Atten-  tion Mechanism을 기반으로 한다. Transformer는 크게  인코더와 디코더로 구성되는데, 인코더는 주어진 텍 스트를 이해하는 역할을 하고 디코더는 이해한 텍스 트를 기반으로 언어를 생성해내는 역할을 수행한다.\n",
      "\n",
      "Source: documents/초거대 언어모델 연구 동향.pdf | Page: 0\n",
      "Content: 리 하드웨어의 개발은 모델 학습에 있어 병목 현상을  크게 완화시켰다. 이로 인해 연구자들은 모델의 복잡 성을 키우고, 더욱 깊은 신경망 구조를 탐구할 수 있 게 되었다. 셋째, 알고리즘 및 기술의 발전은 LLM의  성능 향상을 주도하였다. Attention 및 Transformer  Architecture의 도입은 연구자들에게 문맥 간의 관계 를 더욱 정교하게 모델링할 수 있는 방법을 제공하였 다 [2, 3]. 이 모든 변화의 중심에는 ‘scaling law’라는  * 정회원 1) https://openai.com/blog/chatgpt 학문적인 통찰이 있다 [4]. 해당 연구에 따르면, 모델 의 크기와 그 성능은 긍정적인 상관 관계를 보인다.  이를 통해 연구자들은 모델의 파라미터 수를 증가시 키면서, 이에 따른 성능 향상을 기술적 진보의 상호  작용에서 나온 결과이며, 이러한 추세는 앞으로도  NLP 연구의 주요 동력이 될 것으로 예상된다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 리트리버 테스트\n",
    "query = \"RAG에 대해 이야기해주세요.\"\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\")\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(f\"Source: {result.metadata['source']} | Page: {result.metadata['page']}\")\n",
    "    print(f\"Content: {result.page_content.replace('\\n', ' ')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 프롬프트 템플릿 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "contextual_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Answer the question using only the following context.\"),\n",
    "        (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. RAG 체인 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 개선 전 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug Output: RAG에 대해 이야기해주세요.\n",
      "Debug Output: {'context': [Document(metadata={'source': 'documents/초거대 언어모델 연구 동향.pdf', 'page': 8}, page_content='16 특집원고  초거대 언어모델 연구 동향\\nRetrieval Augmented Generation (RAG) [95, 96, 97, \\n98]이라 한다.\\nOther Tools L a M D A  [ 4 4 ]는 대화 애플리케이션에 \\n특화된 LLM으로, 검색 모듈, 계산기 및 번역기 등의 \\n외부 도구 호출 기능을 가지고 있다. WebGPT [99] 는 \\n웹 브라우저와의 상호작용을 통해 검색 쿼리에 사실 \\n기반의 답변과 함께 출처 정보를 제공 한다. PAL \\n[100]은 Python 인터프리터를 통한 복잡한 기호 추론 \\n기능을 제공하며, 여러 관련 벤치마크에서 뛰어난 성\\n능을 보여주었다. 다양한 종류의 API (e.g., 계산기, 달\\n력, 검색, QA, 번역 등 단순한 API에서부터 Torch/ \\nTensorFlow/HuggingFace Hub에 이르는 복잡한 API까\\n지) 호출 기능을 갖춘 연구들 [101, 102, 103, 104,'), Document(metadata={'source': 'documents/초거대 언어모델 연구 동향.pdf', 'page': 1}, page_content='같은 문자라면 같은 밀집 벡터로 표현하기 때문에 문\\n맥 정보를 반영하지 못한다는 한계를 지닌다.\\n문맥기반 언어모델 연구 문맥 정보를 반영하여 언\\n어를 표현하기 위해, 텍스트 내의 정보를 이용하는 \\nRNN (Recurrent Neural Network) 이 등장했다. 그러나, \\nRNN은 입력 텍스트의 길이가 길어질수록 앞쪽에 위'), Document(metadata={'source': 'documents/초거대 언어모델 연구 동향.pdf', 'page': 2}, page_content='계를 그대로 가진다: 1) 하나의 벡터에 텍스트의 모든 \\n정보를 담기 때문에 정보 손실이 발생하고, 2) 입력 \\n텍스트의 길이가 길어지면 기울기 소실 (gradient \\nvanishing)이 발생한다.\\n이러한 한계를 해결하기 위해 나온 것이 바로 \\nAttention Mechanism [2] 과 이를 활용한 Transformer \\nArchitecture [3] 이다. Attention Mechanism 은 하나의 \\n벡터에 텍스트의 모든 정보를 담는 RNN, LSTM, \\nGRU와 다르게, 텍스트 내 단어들의 벡터들을 필요에 \\n따라 적절히 활용하는 메커니즘이다. 현재 언어모델\\n의 근간이 되는 Transformer가 바로 이러한 Atten- \\ntion Mechanism을 기반으로 한다. Transformer는 크게 \\n인코더와 디코더로 구성되는데, 인코더는 주어진 텍\\n스트를 이해하는 역할을 하고 디코더는 이해한 텍스\\n트를 기반으로 언어를 생성해내는 역할을 수행한다.'), Document(metadata={'source': 'documents/초거대 언어모델 연구 동향.pdf', 'page': 0}, page_content='리 하드웨어의 개발은 모델 학습에 있어 병목 현상을 \\n크게 완화시켰다. 이로 인해 연구자들은 모델의 복잡\\n성을 키우고, 더욱 깊은 신경망 구조를 탐구할 수 있\\n게 되었다. 셋째, 알고리즘 및 기술의 발전은 LLM의 \\n성능 향상을 주도하였다. Attention 및 Transformer \\nArchitecture의 도입은 연구자들에게 문맥 간의 관계\\n를 더욱 정교하게 모델링할 수 있는 방법을 제공하였\\n다 [2, 3]. 이 모든 변화의 중심에는 ‘scaling law’라는 \\n* 정회원\\n1) https://openai.com/blog/chatgpt\\n학문적인 통찰이 있다 [4]. 해당 연구에 따르면, 모델\\n의 크기와 그 성능은 긍정적인 상관 관계를 보인다. \\n이를 통해 연구자들은 모델의 파라미터 수를 증가시\\n키면서, 이에 따른 성능 향상을 기술적 진보의 상호 \\n작용에서 나온 결과이며, 이러한 추세는 앞으로도 \\nNLP 연구의 주요 동력이 될 것으로 예상된다.')], 'question': 'RAG에 대해 이야기해주세요.'}\n",
      "Final Response:\n",
      "RAG, 즉 Retrieval Augmented Generation은 초거대 언어모델 연구의 한 동향으로, 정보 검색과 언어 생성의 결합을 통해 더 나은 응답을 생성하는 방법론입니다. RAG는 외부의 정보 검색 모듈을 활용하여 사용자 쿼리에 대한 사실 기반의 답변을 제공하며, 이러한 방식을 통해 문맥 정보를 더욱 효과적으로 반영할 수 있습니다. RAG는 대화 애플리케이션에서의 활용 가능성과 함께, 다양한 API 호출 기능을 통해 복잡한 응답 생성을 지원하는 연구와도 관련이 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "class DebugPassThrough(RunnablePassthrough):\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        output = super().invoke(*args, **kwargs)\n",
    "        print(\"Debug Output:\", output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class ContextToText(RunnablePassthrough):\n",
    "    def invoke(self, inputs, config=None, **kwargs):\n",
    "        context_text = [doc.page_content.replace('\\n', ' ') for doc in inputs[\"context\"]]\n",
    "        return {\"context\": context_text, \"question\": inputs[\"question\"]}\n",
    "\n",
    "rag_chain_debug = (\n",
    "    {\"context\": retriever, \"question\": DebugPassThrough()}\n",
    "    | DebugPassThrough()\n",
    "    | ContextToText()\n",
    "    | contextual_prompt\n",
    "    | llm_model\n",
    ")\n",
    "\n",
    "# 테스트\n",
    "response = rag_chain_debug.invoke(\"RAG에 대해 이야기해주세요.\")\n",
    "print(\"Final Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 개선한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RAG_Module.Prompt_Engineering import *\n",
    "\n",
    "PROMPT_BASELINE = \"Answer the question using only the following context.\"\n",
    "REFERENCE_BASELINE = \"check user qestion\"\n",
    "\n",
    "class SimplePassThrough:\n",
    "    def invoke(self, inputs, **kwargs):\n",
    "        return inputs\n",
    "\n",
    "\n",
    "class ContextToPrompt:\n",
    "    def __init__(self, prompt_template):\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "    def invoke(self, inputs):\n",
    "        # 문서 내용을 텍스트로 변환\n",
    "        if isinstance(inputs, list):\n",
    "            context_text = [doc.page_content.replace(\"\\n\", \" \") for doc in inputs]\n",
    "            \n",
    "        else:\n",
    "            context_text = inputs\n",
    "\n",
    "        # 프롬프트 템플릿에 적용\n",
    "        formatted_prompt = self.prompt_template.format_messages(\n",
    "            context=context_text, question=inputs.get(\"question\", \"\")\n",
    "        )\n",
    "        return formatted_prompt\n",
    "\n",
    "\n",
    "# Retriever를 invoke() 메서드로 래핑하는 클래스 정의\n",
    "class RetrieverWrapper:\n",
    "    def __init__(self, retriever):\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def invoke(self, inputs):\n",
    "        if isinstance(inputs, dict):\n",
    "            query = inputs.get(\"question\", \"\")\n",
    "        else:\n",
    "            query = inputs\n",
    "        # 검색 수행\n",
    "        response_docs = self.retriever.invoke(query)\n",
    "        return response_docs\n",
    "\n",
    "\n",
    "def RAGChainMake(VECTOR_STORE, PARAMS, PROMPT=PROMPT_BASELINE, REFERENCE=REFERENCE_BASELINE, **kwargs):\n",
    "    \"\"\"\n",
    "    RAG 기법을 이용한 대화형 LLM 답변 체인 생성 (히스토리 기억 및 동적 대화 기능 포함)\n",
    "\n",
    "    VECTOR_STORE : Retriever가 검색할 벡터 스토어\n",
    "    PARAMS       : API Key 및 LLM 모델명 등의 환경 변수 포함\n",
    "    PROMPT       : 시스템 초기 프롬프트 (기본값 설정)\n",
    "    REFERENCE    : 추가 문맥 정보 (선택 사항)\n",
    "    \"\"\"\n",
    "    # 벡터 스토어에서 유사한 문맥 검색\n",
    "    retriever = VECTOR_STORE.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 1}\n",
    "    )\n",
    "\n",
    "    # API 키 설정\n",
    "    openai.api_key = os.environ.get(PARAMS.KEY)\n",
    "    llm_model = ChatOpenAI(\n",
    "        model=PARAMS.LLM_MODEL,\n",
    "        api_key=openai.api_key,\n",
    "    )\n",
    "\n",
    "    # 대화형 프롬프트 생성\n",
    "    contextual_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", f'{PROMPT} \\n\\n reference : {REFERENCE}'),\n",
    "            (\"user\", \"Context: {context}\\n\\nQuestion: {question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # RAG 체인 설정\n",
    "    rag_chain_debug = {\n",
    "        \"context\": RetrieverWrapper(retriever),\n",
    "        \"prompt\": ContextToPrompt(contextual_prompt),\n",
    "        \"llm\": llm_model,\n",
    "    }\n",
    "\n",
    "    return rag_chain_debug\n",
    "\n",
    "\n",
    "def RAG_Coversation(CHAIN, PARAMS, **kwargs):\n",
    "    \"\"\"\n",
    "    사용자로부터 질문을 받아 RAG 체인 기반으로 답변을 생성하는 대화형 함수\n",
    "    전체 대화 결과를 리스트에 저장\n",
    "    \"\"\"\n",
    "    print(\"대화를 시작합니다. 종료하려면 'exit'를 입력하세요.\\n\")\n",
    "\n",
    "    conversation_history = []  # 대화 기록을 저장할 리스트\n",
    "\n",
    "    while True:\n",
    "        print(\"========================\")\n",
    "        query = input(\"질문을 입력하세요 : \")\n",
    "\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"대화를 종료합니다.\")\n",
    "            break\n",
    "\n",
    "        # 1. Retriever로 관련 문서 검색\n",
    "        response_docs = CHAIN[\"context\"].invoke({\"question\": query})\n",
    "\n",
    "        # 2. 문서를 프롬프트로 변환\n",
    "        prompt_messages = CHAIN[\"prompt\"].invoke(\n",
    "            {\"context\": response_docs, \"question\": query}\n",
    "        )\n",
    "\n",
    "        # 3. LLM으로 응답 생성\n",
    "        response = CHAIN[\"llm\"].invoke(prompt_messages)\n",
    "\n",
    "        print(\"\\n답변:\")\n",
    "        print(response.content)\n",
    "\n",
    "        conversation_history.append({\"question\": query, \"answer\": response.content})\n",
    "\n",
    "    while True:\n",
    "        save_result = input(\"\\n결과를 저장하시겠습니까? (y/n): \").strip().lower()\n",
    "\n",
    "        if save_result == \"y\":\n",
    "            PromptResult(conversation_history, PARAMS, **kwargs)\n",
    "            print(\"결과가 저장되었습니다.\")\n",
    "            break  \n",
    "        elif save_result == \"n\":\n",
    "            print(\"결과가 저장되지 않았습니다. 대화를 종료합니다.\")\n",
    "            break  \n",
    "        else:\n",
    "            print(\n",
    "                \"잘못된 입력입니다. 다시 입력해주세요.\"\n",
    "            ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. 챗봇 구동 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load PDF.....\n",
      "Load Complete....!\n",
      "Split docs.....\n",
      "Split Complete....!\n",
      "\n",
      "Authenticate API KEY....\n",
      "Authenticate Complete....!\n",
      "Set up Embedding model....\n",
      "Set up Complete....!\n",
      "Initiate FAISS instance....\n",
      "Return Vector Store....!\n",
      "\n",
      "load Prompt Template....\n",
      "\n",
      "    필요 없는 부분은 공백('')으로 작성\n",
      "    영어 사용, 명령어 사용\n",
      "    리스트, 마크다운 작성 방식으로 성능을 향상 가능\n",
      "\n",
      "    PERSONA : LLM이 수행할 역할 지정\n",
      "    LANG : 답변 생성 언어\n",
      "    TONE : 답변의 어조 설정\n",
      "    PURPOSE : 목적 명시\n",
      "    HOW_WRITE : 답변 방식 예) 개조식\n",
      "    CONDITION : 추가할 조건\n",
      "    REFERENCE : 참조\n",
      "    \n",
      "Making Prompt... \n",
      "\n",
      "\n",
      "    persona : specialist of large language model\n",
      "    language : only in korean\n",
      "    tone : professional\n",
      "    purpose : study large language model\n",
      "    how to write : itemization\n",
      "    condition : \n",
      "\n",
      "    answer is about large language model\n",
      "    prioritize the context of the question\n",
      "    specify if there are any new information\n",
      "    If you can identify the original source of the document you referenced, write in APA format\n",
      "\n",
      "    \n",
      "    reference : omit\n",
      "        \n",
      "대화를 시작합니다. 종료하려면 'exit'를 입력하세요.\n",
      "\n",
      "========================\n",
      "\n",
      "답변:\n",
      "- 업스테이지의 Solar 모델은 Llama2를 파인튜닝하여 개발된 Solar-0-70b 모델입니다.\n",
      "- Solar 모델은 글로벌 LLM 플랫폼 중 하나인 Poe.com에서 서비스되고 있습니다.\n",
      "- Solar 모델은 한국어와 영어를 모두 지원하고 있습니다.\n",
      "========================\n",
      "대화를 종료합니다.\n",
      "결과를 Results/test_prompt_result_24_11191013.json에 저장 중입니다...\n",
      "결과가 Results/test_prompt_result_24_11191013.json에 json 형식으로 저장되었습니다.\n",
      "\n",
      "결과가 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from RAG_Module.RAG_Params import *\n",
    "from RAG_Module.PDF_Loader import PDFLoader\n",
    "from RAG_Module.VecotorStore_Utils import VectorStoreReturn\n",
    "from RAG_Module.RAG_Chain import *\n",
    "from RAG_Module.Prompt_Engineering import *\n",
    "\n",
    "rag_setting = RAGParams(\n",
    "    KEY=\"MY_OPENAI_API_KEY\",\n",
    "    EBD_MODEL=\"text-embedding-ada-002\",\n",
    "    LLM_MODEL=\"gpt-3.5-turbo\",\n",
    "    PDF_PATH=\"documents/초거대 언어모델 연구 동향.pdf\",\n",
    "    SAVE_PATH=None,\n",
    "    IS_SAFE=True,\n",
    "    CHUNK_SIZE=500,\n",
    "    CHUNK_OVERLAP=100,\n",
    ")\n",
    "\n",
    "prompt_setting = PromptParams(\n",
    "    KEY=\"MY_OPENAI_API_KEY\",\n",
    "    LLM_MODEL=\"gpt-3.5-turbo\",\n",
    "    PROMPT_PATH=\"Prompts/\",\n",
    "    PROMPT_NAME=\"test_prompt\",\n",
    "    PROMPT_EXTENSION=\"txt\",\n",
    "    RESULT_PATH=\"Results/\",\n",
    "    RESULT_EXTENSION=\"txt\",\n",
    ")\n",
    "\n",
    "docs = PDFLoader(rag_setting)\n",
    "vector_store = VectorStoreReturn(docs, rag_setting)\n",
    "\n",
    "template_setting = TemplateParams(\n",
    "    PERSONA=\"specialist of large language model\",\n",
    "    LANG=\"only in korean\",\n",
    "    TONE=\"professional\",\n",
    "    PERPOSE=\"study large language model\",\n",
    "    HOW_WRITE=\"itemization\",\n",
    "    CONDITION=\"\"\"\n",
    "\n",
    "    answer is about large language model\n",
    "    prioritize the context of the question\n",
    "    specify if there are any new information\n",
    "    If you can identify the original source of the document you referenced, write in APA format\n",
    "\n",
    "    \"\"\",\n",
    "    REFERENCE= \"context given in the question\",\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(template_setting)\n",
    "chatbot_mk3 = RAGChainMake(vector_store, rag_setting, prompt)\n",
    "RAG_Coversation(chatbot_mk3, prompt_setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\n",
    "    {\n",
    "        \"question\": \"업스테이지의 solar 모델에 대해 설명해줘\",\n",
    "        \"answer\": \"- 업스테이지의 Solar 모델은 Llama2를 파인튜닝하여 개발된 Solar-0-70b 모델입니다.\\n- Solar 모델은 글로벌 LLM 플랫폼 중 하나인 Poe.com에서 서비스되고 있습니다.\\n- Solar 모델은 한국어와 영어를 모두 지원하고 있습니다.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "업스테이지의 Solar 모델은 한국의 여성 가수이자 댄서인 솔라(Solar)의 이름을 딴 제품입니다. Solar 모델은 업스테이지의 다양한 제품 중 하나로, 솔라의 이미지와 스타일을 반영하여 디자인되었습니다. 이 제품은 솔라의 개성을 살린 컬러나 디자인으로 구성되어 있으며, 솔라의 음악과 무대 퍼포먼스와 어울리는 메이크업을 연출할 수 있도록 제작되었습니다. Solar 모델은 솔라의 팬이나 메이크업을 즐기는 이들에게 특별한 즐거움을 주는 제품으로 소개되고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "question = \"업스테이지의 solar 모델에 대해 설명해줘.\"\n",
    "gpt_3 = LLMSupport(question, prompt_setting)\n",
    "print(gpt_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 답변 비교\n",
    "- 질문 : 업스테이지의 solar 모델에 대해 설명해줘.\n",
    "- 확인 사항 : solar 모델은 2023년에 개발된 업스테이지의 최신 모델\n",
    "\n",
    "### GPT-3.5-turbo\n",
    "- 최신 정보를 반영하지 못하고 할루시네이션 발생\n",
    "\n",
    "```\n",
    "업스테이지의 Solar 모델은 한국의 여성 가수이자 댄서인 솔라(Solar)의 이름을 딴 제품입니다. Solar 모델은 업스테이지의 다양한 제품 중 하나로, 솔라의 이미지와 스타일을 반영하여 디자인되었습니다. 이 제품은 솔라의 개성을 살린 컬러나 디자인으로 구성되어 있으며, 솔라의 음악과 무대 퍼포먼스와 어울리는 메이크업을 연출할 수 있도록 제작되었습니다. Solar 모델은 솔라의 팬이나 메이크업을 즐기는 이들에게 특별한 즐거움을 주는 제품으로 소개되고 있습니다\n",
    "```\n",
    "\n",
    "### GPT-4o-mini\n",
    "- 최신 정보를 반영하여 solar 모델에 대한 정보를 제공\n",
    "- 상세한 정보를 반영하고 있지 않고 대략적인 정보만 제공\n",
    "\n",
    "```\n",
    "업스테이지(Upstage)의 Solar 모델은 자연어 처리(NLP)와 관련된 AI 모델 중 하나로, 주로 대화형 AI 시스템이나 문서 이해, 질의응답 시스템 등 다양한 언어 기반 작업을 수행할 수 있는 고성능 언어 모델입니다. Solar 모델은 대규모 데이터셋을 기반으로 학습된 트랜스포머 아키텍처를 사용하며, 한국어와 영어를 포함한 다양한 언어에서 뛰어난 성능을 발휘할 수 있도록 최적화되어 있습니다.\n",
    "\n",
    "Solar 모델의 특징:\n",
    "\n",
    "대규모 언어 모델: 대량의 텍스트 데이터를 학습하여, 다양한 언어적 상황에서 의미를 파악하고 이해할 수 있습니다.\n",
    "트랜스포머 기반: Transformer 아키텍처를 기반으로 하기 때문에 문맥을 잘 이해하고, 긴 텍스트나 복잡한 문장 구조도 효과적으로 처리할 수 있습니다.\n",
    "다양한 활용 가능성: 문서 요약, 감정 분석, 기계 번역, 대화형 AI, 질의응답 시스템 등 여러 가지 언어적 작업에서 활용될 수 있습니다.\n",
    "한국어 최적화: 업스테이지는 특히 한국어에 최적화된 모델을 제공하고 있어, 한국어 처리 성능이 뛰어나고, 한국어를 사용하는 다양한 실시간 응용 프로그램에 효과적입니다.\n",
    "이 모델은 업스테이지의 다른 AI 시스템들과 함께 사용되며, 기업 고객이나 연구자들이 효율적으로 텍스트 기반의 문제를 해결하는 데 도움을 줍니다\n",
    "```\n",
    "\n",
    "### RAG + GPT-3.5-turbo\n",
    "- GPT-3.5-turbo에 없는 정보를 RAG 검색을 이용하여 답변\n",
    "- 업스테이지에서 작성한 문서르 참고했기 떄문에 모델명 등의 상세한 정보를 제공\n",
    "\n",
    "```\n",
    "- 업스테이지의 Solar 모델은 Llama2를 파인튜닝하여 개발된 Solar-0-70b 모델입니다.\n",
    "- Solar 모델은 글로벌 LLM 플랫폼 중 하나인 Poe.com에서 서비스되고 있습니다.\n",
    "- Solar 모델은 한국어와 영어를 모두 지원하고 있습니다.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 도전 구현 과제\n",
    "1. LangSmith의 Prompt Library를 참고하여 prompt engineering을 수행\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " i) LangSmith의 Prompt Library 를 참고하여 프롬프트를 3개 이상 아래와 같은 파일 구조로 저장해주세요. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ii) 각 프롬프트를 외부에서 불러와서 실행할 수 있도록 코드를 고쳐주세요. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " iii) 실행 결과는 자동으로 Result 디렉토리에 저장되어야 합니다. 이때, 실험 결과 파일 이름은 실험에 쓰인 프롬프트의 이름과 timestamp을 포함해야합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load PDF.....\n",
      "Load Complete....!\n",
      "Split docs.....\n",
      "Split Complete....!\n",
      "\n",
      "Authenticate API KEY....\n",
      "Authenticate Complete....!\n",
      "Set up Embedding model....\n",
      "Set up Complete....!\n",
      "Initiate FAISS instance....\n",
      "Return Vector Store....!\n",
      "\n",
      "prompt_1 이/가 Prompts/ 에 저장되었습니다.\n",
      "\n",
      "prompt_2 이/가 Prompts/ 에 저장되었습니다.\n",
      "\n",
      "load Prompt Template....\n",
      "\n",
      "    필요 없는 부분은 공백('')으로 작성\n",
      "    영어 사용, 명령어 사용\n",
      "    리스트, 마크다운 작성 방식으로 성능을 향상 가능\n",
      "\n",
      "    PERSONA : LLM이 수행할 역할 지정\n",
      "    LANG : 답변 생성 언어\n",
      "    TONE : 답변의 어조 설정\n",
      "    PURPOSE : 목적 명시\n",
      "    HOW_WRITE : 답변 방식 예) 개조식\n",
      "    CONDITION : 추가할 조건\n",
      "    REFERENCE : 참조\n",
      "    \n",
      "Making Prompt... \n",
      "\n",
      "\n",
      "    persona : specialist of large language model\n",
      "    language : only in korean\n",
      "    tone : professional\n",
      "    purpose : study large language model\n",
      "    how to write : itemization\n",
      "    condition : \n",
      "\n",
      "    <must obey>\n",
      "    answer is about large language model\n",
      "    answer that you do not know what you do not know\n",
      "    </must obey>\n",
      "\n",
      "    if you canspecify the date standard of the information\n",
      "    if you can identify the original source of the document you referenced, write in APA format\n",
      "    \n",
      "    reference : omit\n",
      "        \n",
      "load Prompt Template....\n",
      "\n",
      "    필요 없는 부분은 공백('')으로 작성\n",
      "    영어 사용, 명령어 사용\n",
      "    리스트, 마크다운 작성 방식으로 성능을 향상 가능\n",
      "\n",
      "    PERSONA : LLM이 수행할 역할 지정\n",
      "    LANG : 답변 생성 언어\n",
      "    TONE : 답변의 어조 설정\n",
      "    PURPOSE : 목적 명시\n",
      "    HOW_WRITE : 답변 방식 예) 개조식\n",
      "    CONDITION : 추가할 조건\n",
      "    REFERENCE : 참조\n",
      "    \n",
      "Making Prompt... \n",
      "\n",
      "\n",
      "    persona : specialist of large language model\n",
      "    language : only in korean\n",
      "    tone : professional\n",
      "    purpose : study large language model\n",
      "    how to write : itemization\n",
      "    condition : \n",
      "\n",
      "    <must obey>\n",
      "    answer is about large language model\n",
      "    answer that you do not know what you do not know\n",
      "    </must obey>\n",
      "\n",
      "    prioritize the context in question\n",
      "    specify if there are any new information\n",
      "\n",
      "    if you can identify the original source of the document you referenced, write in APA format\n",
      "    \n",
      "    reference : omit\n",
      "        \n",
      "prompt_3 이/가 Prompts/ 에 저장되었습니다.\n",
      "\n",
      "\n",
      "prompt_2.txt 로 시작합니다.\n",
      "prompt_2 이/가 Prompts/ 에서 불러와졌습니다.\n",
      "질문: 업스테이지의 solar 모델에 대해 설명해줘.\n",
      "답변: 업스테이지의 Solar-0-70b 모델은 Llama2를 파인튜닝하여 개발된 한국어 LLM입니다. 이 모델은 한국어와 영어를 모두 지원하며, 글로벌 LLM 플랫폼 중 하나인 Poe.com에서 서비스되고 있습니다.\n",
      "결과를 Results/prompt_2_result_24_11191038.json에 저장 중입니다...\n",
      "결과가 Results/prompt_2_result_24_11191038.json에 json 형식으로 저장되었습니다.\n",
      "\n",
      "prompt_2.txt에 대한 결과 저장 완료.\n",
      "\n",
      "prompt_3.txt 로 시작합니다.\n",
      "prompt_3 이/가 Prompts/ 에서 불러와졌습니다.\n",
      "질문: 업스테이지의 solar 모델에 대해 설명해줘.\n",
      "답변: 업스테이지의 Solar 모델에 대한 설명은 다음과 같습니다:\n",
      "\n",
      "1. **모델 개요**:\n",
      "   - Solar 모델은 업스테이지에서 개발한 대규모 언어 모델입니다.\n",
      "   - Llama2를 기반으로 파인튜닝되어 생성되었습니다.\n",
      "\n",
      "2. **지원 언어**:\n",
      "   - Solar 모델은 한국어와 영어를 모두 지원합니다.\n",
      "\n",
      "3. **응용 분야**:\n",
      "   - 해당 모델은 글로벌 LLM 플랫폼인 Poe.com에서 서비스되고 있습니다.\n",
      "\n",
      "4. **훈련 데이터**:\n",
      "   - 한국어 데이터와 공개된 한국어 데이터, 크롤링 데이터를 활용하여 학습하였습니다.\n",
      "\n",
      "5. **모델의 특징**:\n",
      "   - 한국어 토큰 비율을 높여 한국어 처리 성능을 개선하는 데 중점을 두고 있습니다.\n",
      "\n",
      "이 정보는 제공된 문서의 내용을 바탕으로 작성되었습니다. (초거대 언어모델 연구 동향.pdf, p. 3)\n",
      "결과를 Results/prompt_3_result_24_11191038.json에 저장 중입니다...\n",
      "결과가 Results/prompt_3_result_24_11191038.json에 json 형식으로 저장되었습니다.\n",
      "\n",
      "prompt_3.txt에 대한 결과 저장 완료.\n",
      "\n",
      "prompt_1.txt 로 시작합니다.\n",
      "prompt_1 이/가 Prompts/ 에서 불러와졌습니다.\n",
      "질문: 업스테이지의 solar 모델에 대해 설명해줘.\n",
      "답변: 업스테이지는 Llama2를 파인튜닝하여 Solar-0-70b 모델을 개발하였으며, 이 모델은 한국어와 영어 모두 지원하는 글로벌 LLM 플랫폼 중 하나인 Poe.com에서 서비스되고 있습니다.\n",
      "결과를 Results/prompt_1_result_24_11191038.json에 저장 중입니다...\n",
      "결과가 Results/prompt_1_result_24_11191038.json에 json 형식으로 저장되었습니다.\n",
      "\n",
      "prompt_1.txt에 대한 결과 저장 완료.\n",
      "모든 결과가 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 세팅\n",
    "\n",
    "rag_setting = RAGParams(\n",
    "    KEY=\"MY_OPENAI_API_KEY\",\n",
    "    EBD_MODEL=\"text-embedding-ada-002\",\n",
    "    LLM_MODEL=\"gpt-3.5-turbo\",\n",
    "    PDF_PATH=\"documents/초거대 언어모델 연구 동향.pdf\",\n",
    "    SAVE_PATH=None,\n",
    "    IS_SAFE=True,\n",
    "    CHUNK_SIZE=500,\n",
    "    CHUNK_OVERLAP=100,\n",
    ")\n",
    "\n",
    "prompt_setting = PromptParams(\n",
    "    KEY=\"MY_OPENAI_API_KEY\",\n",
    "    LLM_MODEL=\"gpt-4o-mini\",\n",
    "    PROMPT_PATH=\"Prompts/\",\n",
    "    PROMPT_NAME=None,\n",
    "    PROMPT_EXTENSION=\"txt\",\n",
    "    RESULT_PATH=\"Results/\",\n",
    "    RESULT_EXTENSION=\"txt\",\n",
    ")\n",
    "\n",
    "# 문서 불러오기 및 분할\n",
    "docs = PDFLoader(rag_setting)\n",
    "\n",
    "# 벡터 스토어 생성\n",
    "vector_store = VectorStoreReturn(docs, rag_setting)\n",
    "\n",
    "# 프롬프트 1 작성 및 저장\n",
    "prompt_1 = \"Answer the question using only the following context.\"\n",
    "PromptSave(prompt_1, prompt_setting, PROMPT_NAME='prompt_1')\n",
    "\n",
    "# 프롬프트 2 작성 및 저장\n",
    "prompt_2 = \"\"\"\n",
    "\n",
    "you are specialist of large language model\n",
    "answer question\n",
    "refer to context qiven in question\n",
    "\n",
    "\"\"\"\n",
    "PromptSave(prompt_2, prompt_setting, PROMPT_NAME='prompt_2')\n",
    "\n",
    "# 프롬프트 3 작성 및 저장\n",
    "\n",
    "## shot 기법 사용을 위한 shot 제작용 프롬프트 생성\n",
    "shot_template = TemplateParams(\n",
    "    PERSONA=\"specialist of large language model\",\n",
    "    LANG=\"only in korean\",\n",
    "    TONE=\"professional\",\n",
    "    PERPOSE=\"study large language model\",\n",
    "    HOW_WRITE=\"itemization\",\n",
    "    CONDITION=\"\"\"\n",
    "\n",
    "    <must obey>\n",
    "    answer is about large language model\n",
    "    answer that you do not know what you do not know\n",
    "    </must obey>\n",
    "\n",
    "    if you canspecify the date standard of the information\n",
    "    if you can identify the original source of the document you referenced, write in APA format\n",
    "    \"\"\",\n",
    "    REFERENCE=\"only the latest information\")\n",
    "shot_prompt = PromptTemplate(shot_template)\n",
    "\n",
    "## shot 생성 - 상위 모델인 gpt-4o-mini 의 답변 방식을 참고하도록 유도\n",
    "question = \"gpt-4에 대해서 설명해줘\"\n",
    "shot = LLMSupport(question, prompt_setting, shot_prompt)\n",
    "\n",
    "## 프롬프트 3 작성 및 저장\n",
    "template_setting = TemplateParams(\n",
    "    PERSONA=\"specialist of large language model\",\n",
    "    LANG=\"only in korean\",\n",
    "    TONE=\"professional\",\n",
    "    PERPOSE=\"study large language model\",\n",
    "    HOW_WRITE=\"itemization\",\n",
    "    CONDITION=\"\"\"\n",
    "\n",
    "    <must obey>\n",
    "    answer is about large language model\n",
    "    answer that you do not know what you do not know\n",
    "    </must obey>\n",
    "\n",
    "    prioritize the context in question\n",
    "    specify if there are any new information\n",
    "\n",
    "    if you can identify the original source of the document you referenced, write in APA format\n",
    "    \"\"\",\n",
    "    REFERENCE=f\"\"\"\n",
    "\n",
    "    <answer format sample>\n",
    "    {shot}\n",
    "    </answer format>\n",
    "    \n",
    "    refer to context given in the question\",\n",
    "    \"\"\"\n",
    ")\n",
    "prompt_3 = PromptTemplate(template_setting)\n",
    "PromptSave(prompt_3, prompt_setting, PROMPT_NAME='prompt_3')\n",
    "\n",
    "# 저장된 프롬프트 1, 2, 3 불러오기 및 결과 저장\n",
    "## QUESTION 리스트 형태로 입력\n",
    "QUESTION = [\"업스테이지의 solar 모델에 대해 설명해줘.\",]\n",
    "AutoChain(prompt_setting, vector_store, QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "LLM_RAG/\n",
    "│\n",
    "├── Prompts/\n",
    "│   ├── prompt_1.txt\n",
    "│   ├── prompt_2.txt\n",
    "│   └── prompt_3.txt\n",
    "└── Results/\n",
    "    ├── prompt_1_result_24_11191038.json\n",
    "    ├── prompt_2_result_24_11191038.json\n",
    "    └── prompt_3_result_24_11191038.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question\n",
    "```\n",
    " 업스테이지의 solar 모델에 대해 설명해줘\n",
    "```\n",
    "\n",
    "## prompt 1\n",
    "### prompt\n",
    "\n",
    "```\n",
    "Answer the question using only the following context.\n",
    "```\n",
    "\n",
    "### answer\n",
    "\n",
    "```\n",
    "\"업스테이지는 Llama2를 파인튜닝하여 Solar-0-70b 모델을 개발하였으며, 이 모델은 한국어와 영어 모두 지원하는 글로벌 LLM 플랫폼 중 하나인 Poe.com에서 서비스되고 있습니다.\"\n",
    "```\n",
    "\n",
    "## prompt 2\n",
    "### prompt\n",
    "\n",
    "```\n",
    "you are specialist of large language model\n",
    "answer question\n",
    "refer to context qiven in question\n",
    "```\n",
    "\n",
    "### answer\n",
    "\n",
    "```\n",
    "\"업스테이지의 Solar-0-70b 모델은 Llama2를 파인튜닝하여 개발된 한국어 LLM입니다. 이 모델은 한국어와 영어를 모두 지원하며, 글로벌 LLM 플랫폼 중 하나인 Poe.com에서 서비스되고 있습니다.\"\n",
    "```\n",
    "\n",
    "## prompt 3\n",
    "### prompt\n",
    "\n",
    "```\n",
    "    persona : specialist of large language model\n",
    "    language : only in korean\n",
    "    tone : professional\n",
    "    purpose : study large language model\n",
    "    how to write : itemization\n",
    "    condition : \n",
    "\n",
    "    <must obey>\n",
    "    answer is about large language model\n",
    "    answer that you do not know what you do not know\n",
    "    </must obey>\n",
    "\n",
    "    prioritize the context in question\n",
    "    specify if there are any new information\n",
    "\n",
    "    if you can identify the original source of the document you referenced, write in APA format\n",
    "    \n",
    "    reference : \n",
    "\n",
    "    <answer format sample>\n",
    "    GPT-4에 대한 설명은 다음과 같습니다:\n",
    "\n",
    "   1. **모델 개요**:\n",
    "      - GPT-4는 OpenAI에서 개발한 대규모 언어 모델로, 자연어 처리(NLP) 작업을 수행하는 데 사용됩니다.\n",
    "      - 이전 버전인 GPT-3에 비해 더 많은 파라미터와 개선된 알고리즘을 통해 성능이 향상되었습니다.\n",
    "\n",
    "   2. **기능**:\n",
    "      - 텍스트 생성: 주어진 프롬프트에 따라 자연스러운 문장을 생성할 수 있습니다.\n",
    "      - 질문 응답: 사용자의 질문에 대해 관련 정보를 바탕으로 답변을 제공합니다.\n",
    "      - 번역: 여러 언어 간의 번역 작업을 수행할 수 있습니다.\n",
    "      - 요약: 긴 텍스트를 간결하게 요약하는 기능을 갖추고 있습니다.\n",
    "\n",
    "   3. **훈련 데이터**:\n",
    "      - GPT-4는 다양한 출처의 대규모 텍스트 데이터로 훈련되었습니다. 이는 웹사이트, 책, 논문 등 다양한 형식의 데이터를 포함합니다.\n",
    "      - 훈련 데이터는 2023년 10월까지의 정보로 제한되어 있습니다.\n",
    "\n",
    "   4. **응용 분야**:\n",
    "      - 고객 서비스: 챗봇 및 가상 비서로 활용됩니다.\n",
    "      - 콘텐츠 생성: 블로그, 기사, 소설 등 다양한 콘텐츠를 생성하는 데 사용됩니다.\n",
    "      - 교육: 학습 도구로 활용되어 학생들에게 정보를 제공합니다.\n",
    "\n",
    "   5. **한계**:\n",
    "      - GPT-4는 훈련 데이터에 기반하여 작동하므로, 최신 정보나 특정 전문 분야에 대한 깊이 있는 지식이 부족할 수 있습니다.\n",
    "      - 모델이 생성하는 정보의 정확성이나 신뢰성은 항상 보장되지 않으며, 사용자는 결과를 비판적으로 평가해야 합니다.\n",
    "\n",
    "   6. **미래 전망**:\n",
    "      - GPT-4와 같은 모델은 지속적으로 발전하고 있으며, 향후 더 나은 성능과 다양한 기능을 갖춘 모델이 개발될 것으로 예상됩니다.\n",
    "\n",
    "   이 정보는 OpenAI의 공식 발표 및 연구 자료를 바탕으로 작성되었습니다. (OpenAI, 2023)\n",
    "    </answer format>\n",
    "    \n",
    "    refer to context given in the question\",\n",
    "```\n",
    "\n",
    "### answer\n",
    "\n",
    "```\n",
    "\"업스테이지의 Solar 모델에 대한 설명은 다음과 같습니다:\n",
    "1. **모델 개요**:\n",
    "   - Solar 모델은 업스테이지에서 개발한 대규모 언어 모델입니다.\n",
    "   - Llama2를 기반으로 파인튜닝되어 생성되었습니다.\n",
    "      \n",
    "2. **지원 언어**:\n",
    "   - Solar 모델은 한국어와 영어를 모두 지원합니다.\n",
    "\n",
    "3. **응용 분야**:\n",
    "   - 해당 모델은 글로벌 LLM 플랫폼인 Poe.com에서 서비스되고 있습니다.\n",
    "   \n",
    "4. **훈련 데이터**:\n",
    "   - 한국어 데이터와 공개된 한국어 데이터, 크롤링 데이터를 활용하여 학습하였습니다.\n",
    "   \n",
    "5. **모델의 특징**:\n",
    "   - 한국어 토큰 비율을 높여 한국어 처리 성능을 개선하는 데 중점을 두고 있습니다.\n",
    "   \n",
    "이 정보는 제공된 문서의 내용을 바탕으로 작성되었습니다. (초거대 언어모델 연구 동향.pdf, p. 3)\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
