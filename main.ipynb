{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 환경 설정\n",
    "\n",
    "1. 패키지 설치\n",
    "2. vscode 내 jupyter notebook 환경에서 input 기능 활성화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필수 구현 과제\n",
    "1. 사용 환경 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# 환경 변수에서 호출\n",
    "openai.api_key = os.environ.get(\"MY_OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 모델 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 모델\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\", api_key=openai.api_key\n",
    ")\n",
    "\n",
    "# LLM 모델\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_model = ChatOpenAI(model=\"gpt-4o-mini\", api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 문서 로드하기\n",
    "- 사용 문서\n",
    "  - 초거대 언어모델 연구 동향.pdf\n",
    "  - 박찬준 외, 「초거대 언어모델 연구 동향」, 『정보학회지』, 제41권 제11호(통권 제414호), 한국정보과학회, 2023, 8-24 \n",
    "  - 출처 [¶](https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11610124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "페이지 수: 17\n",
      "\n",
      "[페이지내용]\n",
      "8 특집원고  초거대 언어모델 연구 동향\n",
      "초거대 언어모델 연구 동향\n",
      "업스테이지  박찬준*･이원성･김윤기･김지후･이활석\n",
      " \n",
      "1. 서  론1)\n",
      "ChatGPT1)와 같은 초거대 언어모델(Large Language \n",
      "Model, LLM) 의 등장으로 기존에 병렬적으로 연구되\n",
      "던 다양한 자연언어처리 하위 분야들이 하나의 모델\n",
      "로 처리되고 있으며, 태스크 수렴 현상 (Converge)이 \n",
      "발생하고 있다. 즉 하나의 LLM으로 번역, 요약, 질의\n",
      "응답, 형태소분석 등의 작업을 모두 처리할 수 있게 \n",
      "되었다. 프롬프트 (Prompt)를 어떻게 모델에게 입력하\n",
      "느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이\n",
      "에 따라 사용자의 목적에 맞는 출력을 생성하는 패러\n",
      "다임을 맞이하게 되었다 [1].\n",
      "LLM은 최근 몇 년 간의 연구 동향에 따라 뛰어난 \n",
      "발전을 이루고 있다. 이러한 발전은 몇 가지 주요한 \n",
      "요인에 기반하고 있으며, 이 요인들은 현대 자연언어\n",
      "처리 (Natural Language Proce\n",
      "\n",
      "[metadata]\n",
      "{'source': 'documents/초거대 언어모델 연구 동향.pdf', 'page': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# 가져올 pdf 파일 경로\n",
    "path = \"documents/초거대 언어모델 연구 동향.pdf\"\n",
    "\n",
    "# 사용할 pdf loader 선택\n",
    "loader = PyPDFLoader(path)\n",
    "\n",
    "# pdf 파일 불러오기\n",
    "docs = loader.load()\n",
    "\n",
    "# 불러올 범위 설정\n",
    "page = 0\n",
    "start_point = 0\n",
    "end_point = 500\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"페이지 수: {len(docs)}\")\n",
    "print(f\"\\n[페이지내용]\\n{docs[page].page_content[start_point:end_point]}\")\n",
    "print(f\"\\n[metadata]\\n{docs[page].metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 문서 청크로 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정\n",
    "CHUNK_INDEX = 0\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "SEPERATOR = \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i .`CharacterTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_splits\n",
      "길이 : 142\n",
      " 결과 확인 : 8 특집원고  초거대 언어모델 연구 동향\n",
      "초거대 언어모델 연구 동향\n",
      "업스테이지  박찬준*･이원성･김윤기･김지후･이활석\n",
      " \n",
      "1. 서  론1)\n",
      "ChatGPT1)와 같은 초거대 언어모델(Large Language \n",
      "Model, LLM) 의 등장으로 기존에 병렬적으로 연구되\n",
      "던 다양한 자연언어처리 하위 분야들이 하나의 모델\n",
      "로 처리되고 있으며, 태스크 수렴 현상 (Converge)이 \n",
      "발생하고 있다. 즉 하나의 LLM으로 번역, 요약, 질의\n",
      "응답, 형태소분석 등의 작업을 모두 처리할 수 있게 \n",
      "되었다. 프롬프트 (Prompt)를 어떻게 모델에게 입력하\n",
      "느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이\n",
      "에 따라 사용자의 목적에 맞는 출력을 생성하는 패러\n",
      "다임을 맞이하게 되었다 [1].\n",
      "LLM은 최근 몇 년 간의 연구 동향에 따라 뛰어난 \n",
      "발전을 이루고 있다. 이러한 발전은 몇 가지 주요한 \n",
      "요인에 기반하고 있으며, 이 요인들은 현대 자연언어\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 문서 분할기 설정\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=SEPERATOR,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    is_separator_regex=True,\n",
    ")\n",
    "\n",
    "# 문서 분할\n",
    "c_splits = splitter.split_documents(docs)\n",
    "\n",
    "# 결과 확인\n",
    "print(\n",
    "    f\"c_splits\\n길이 : {len(c_splits)}\\n 결과 확인 : {c_splits[CHUNK_INDEX].page_content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii .`RecursiveCharacterTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_splits\n",
      "길이 : 141\n",
      " 결과 확인 : 8 특집원고  초거대 언어모델 연구 동향\n",
      "초거대 언어모델 연구 동향\n",
      "업스테이지  박찬준*･이원성･김윤기･김지후･이활석\n",
      " \n",
      "1. 서  론1)\n",
      "ChatGPT1)와 같은 초거대 언어모델(Large Language \n",
      "Model, LLM) 의 등장으로 기존에 병렬적으로 연구되\n",
      "던 다양한 자연언어처리 하위 분야들이 하나의 모델\n",
      "로 처리되고 있으며, 태스크 수렴 현상 (Converge)이 \n",
      "발생하고 있다. 즉 하나의 LLM으로 번역, 요약, 질의\n",
      "응답, 형태소분석 등의 작업을 모두 처리할 수 있게 \n",
      "되었다. 프롬프트 (Prompt)를 어떻게 모델에게 입력하\n",
      "느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이\n",
      "에 따라 사용자의 목적에 맞는 출력을 생성하는 패러\n",
      "다임을 맞이하게 되었다 [1].\n",
      "LLM은 최근 몇 년 간의 연구 동향에 따라 뛰어난 \n",
      "발전을 이루고 있다. 이러한 발전은 몇 가지 주요한 \n",
      "요인에 기반하고 있으며, 이 요인들은 현대 자연언어\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 문서 분할기 설정\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# 문서 분할\n",
    "r_splits = recursive_splitter.split_documents(docs)\n",
    "\n",
    "# 결과 확인\n",
    "print(\n",
    "    f\"r_splits\\n길이 : {len(r_splits)}\\n 결과 확인 : {r_splits[CHUNK_INDEX].page_content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **CharacterTextSplitter**\n",
    "  - 기본적인 분할 방식\n",
    "  - 구분자를 기준으로 청크 단위로 분할\n",
    "  \n",
    "- **RecursiveCharacterTextSplitter**\n",
    "  - 단락-문장-단어 순서로 재귀적으로 분할\n",
    "  - 여러번의 분할로 작은 덩어리 생성\n",
    "  - 텍스트가 너무 크거나 복잡할 때 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 10개 테스트\n",
    "\n",
    "# def sample_printer(splits_1, splits_2, n):\n",
    "#     for i in range(n):\n",
    "#         print(f\"Sample 생성중...\")\n",
    "#         print(f\"Sample_1 {i} \\n {splits_1[i].page_content}\")\n",
    "#         print(f\"Sample_2 {i} \\n {splits_2[i].page_content}\")\n",
    "\n",
    "# sample_printer(c_splits, r_splits, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 벡터 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\", api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 벡터 스토어 생성\n",
    "- `RecursiveCharacterTextSplitter` 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.345] 16 특집원고  초거대 언어모델 연구 동향\n",
      "Retrieval Augmented Generation (RAG) [95, 96, 97, \n",
      "98]이라 한다.\n",
      "Other Tools L a M D A  [ 4 4 ]는 대화 애플리케이션에 \n",
      "특화된 LLM으로, 검색 모듈, 계산기 및 번역기 등의 \n",
      "외부 도구 호출 기능을 가지고 있다. WebGPT [99] 는 \n",
      "웹 브라우저와의 상호작용을 통해 검색 쿼리에 사실 \n",
      "기반의 답변과 함께 출처 정보를 제공 한다. PAL \n",
      "[100]은 Python 인터프리터를 통한 복잡한 기호 추론 \n",
      "기능을 제공하며, 여러 관련 벤치마크에서 뛰어난 성\n",
      "능을 보여주었다. 다양한 종류의 API (e.g., 계산기, 달\n",
      "력, 검색, QA, 번역 등 단순한 API에서부터 Torch/ \n",
      "TensorFlow/HuggingFace Hub에 이르는 복잡한 API까\n",
      "지) 호출 기능을 갖춘 연구들 [101, 102, 103, 104, [{'source': 'documents/초거대 언어모델 연구 동향.pdf', 'page': 8}]\n",
      "* [SIM=0.424] 같은 문자라면 같은 밀집 벡터로 표현하기 때문에 문\n",
      "맥 정보를 반영하지 못한다는 한계를 지닌다.\n",
      "문맥기반 언어모델 연구 문맥 정보를 반영하여 언\n",
      "어를 표현하기 위해, 텍스트 내의 정보를 이용하는 \n",
      "RNN (Recurrent Neural Network) 이 등장했다. 그러나, \n",
      "RNN은 입력 텍스트의 길이가 길어질수록 앞쪽에 위 [{'source': 'documents/초거대 언어모델 연구 동향.pdf', 'page': 1}]\n"
     ]
    }
   ],
   "source": [
    "# RecursiveCharacterTextSplitter 결과 사용\n",
    "splits = r_splits\n",
    "\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "index = faiss.IndexFlatL2(len(embedding_model.embed_query(\"This is Sample Text.\")))\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embedding_model,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(splits))]\n",
    "vector_store.add_documents(documents=splits, ids=uuids)\n",
    "\n",
    "# 테스트\n",
    "results_with_scores = vector_store.similarity_search_with_score(\n",
    "    \"RAG에 대해 이야기해주세요.\", k=2, filter={\"source\": 'documents/초거대 언어모델 연구 동향.pdf'}\n",
    ")\n",
    "for res, score in results_with_scores:\n",
    "    print(f\"* [SIM={score:.3f}] {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. FAISS를 Retriever로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리트리버 테스트\n",
    "# query = \"RAG에 대해 이야기해주세요.\"\n",
    "\n",
    "# retriever = vector_store.as_retriever(search_type=\"similarity\")\n",
    "# results = retriever.invoke(query)\n",
    "# for result in results:\n",
    "#     print(f\"Source: {result.metadata['source']} | Page: {result.metadata['page']}\")\n",
    "#     print(f\"Content: {result.page_content.replace('\\n', ' ')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 프롬프트 템플릿 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "contextual_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Answer the question using only the following context.\"),\n",
    "        (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. RAG 체인 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePassThrough:\n",
    "    def invoke(self, inputs, **kwargs):\n",
    "        return inputs\n",
    "\n",
    "\n",
    "class ContextToPrompt:\n",
    "    def __init__(self, prompt_template):\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "    def invoke(self, inputs):\n",
    "        # 문서 내용을 텍스트로 변환\n",
    "        if isinstance(inputs, list):\n",
    "            context_text = [doc.page_content.replace(\"\\n\", \" \") for doc in inputs]\n",
    "            \n",
    "        else:\n",
    "            context_text = inputs\n",
    "\n",
    "        # 프롬프트 템플릿에 적용\n",
    "        formatted_prompt = self.prompt_template.format_messages(\n",
    "            context=context_text, question=inputs.get(\"question\", \"\")\n",
    "        )\n",
    "        return formatted_prompt\n",
    "\n",
    "\n",
    "# Retriever를 invoke() 메서드로 래핑하는 클래스 정의\n",
    "class RetrieverWrapper:\n",
    "    def __init__(self, retriever):\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def invoke(self, inputs):\n",
    "        if isinstance(inputs, dict):\n",
    "            query = inputs.get(\"question\", \"\")\n",
    "        else:\n",
    "            query = inputs\n",
    "        # 검색 수행\n",
    "        response_docs = self.retriever.invoke(query)\n",
    "        return response_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_BASELINE = \"Answer the question using only the following context.\"\n",
    "REFERENCE_BASELINE = \"check user qestion\"\n",
    "\n",
    "def RAGChainMake(VECTOR_STORE, KEY, LLM_MODEL='gpt-3.5-turbo', PROMPT=PROMPT_BASELINE, REFERENCE=REFERENCE_BASELINE, **kwargs):\n",
    "    \"\"\"\n",
    "    RAG 기법을 이용한 대화형 LLM 답변 체인 생성 (히스토리 기억 및 동적 대화 기능 포함)\n",
    "\n",
    "    VECTOR_STORE : Retriever가 검색할 벡터 스토어\n",
    "    KEY : API Key\n",
    "    LLM_MODEL : LLM 모델명\n",
    "    PROMPT    : 시스템 초기 프롬프트 (기본값 설정)\n",
    "    REFERENCE : 추가 문맥 정보 (선택 사항)\n",
    "    \"\"\"\n",
    "    # 벡터 스토어에서 유사한 문맥 검색\n",
    "    retriever = VECTOR_STORE.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 1}\n",
    "    )\n",
    "\n",
    "    # API 키 설정\n",
    "    openai.api_key = os.environ.get(KEY)\n",
    "    llm_model = ChatOpenAI(\n",
    "        model=LLM_MODEL,\n",
    "        api_key=openai.api_key,\n",
    "    )\n",
    "\n",
    "    # 대화형 프롬프트 생성\n",
    "    contextual_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", f'{PROMPT} \\n\\n reference : {REFERENCE}'),\n",
    "            (\"user\", \"Context: {context}\\n\\nQuestion: {question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # RAG 체인 설정\n",
    "    rag_chain_debug = {\n",
    "        \"context\": RetrieverWrapper(retriever),\n",
    "        \"prompt\": ContextToPrompt(contextual_prompt),\n",
    "        \"llm\": llm_model,\n",
    "    }\n",
    "\n",
    "    return rag_chain_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. 챗봇 구동 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAG_Coversation(CHAIN, **kwargs):\n",
    "    \"\"\"\n",
    "    사용자로부터 질문을 받아 RAG 체인 기반으로 답변을 생성하는 대화형 함수\n",
    "    전체 대화 결과를 리스트에 저장\n",
    "    \"\"\"\n",
    "    print(\"대화를 시작합니다. 종료하려면 'exit'를 입력하세요.\\n\")\n",
    "\n",
    "    conversation_history = []  # 대화 기록을 저장할 리스트\n",
    "\n",
    "    while True:\n",
    "        print(\"========================\")\n",
    "        query = input(\"질문을 입력하세요 : \")\n",
    "\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"대화를 종료합니다.\")\n",
    "            break\n",
    "\n",
    "        # 1. Retriever로 관련 문서 검색\n",
    "        response_docs = CHAIN[\"context\"].invoke({\"question\": query})\n",
    "\n",
    "        # 2. 문서를 프롬프트로 변환\n",
    "        prompt_messages = CHAIN[\"prompt\"].invoke(\n",
    "            {\"context\": response_docs, \"question\": query}\n",
    "        )\n",
    "\n",
    "        # 3. LLM으로 응답 생성\n",
    "        response = CHAIN[\"llm\"].invoke(prompt_messages)\n",
    "\n",
    "        print(\"\\n답변:\")\n",
    "        print(response.content)\n",
    "\n",
    "        conversation_history.append({\"question\": query, \"answer\": response.content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">질문 = '업스테이지 solar 모델에 대해서 알려주세요.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대화를 시작합니다. 종료하려면 'exit'를 입력하세요.\n",
      "\n",
      "========================\n",
      "\n",
      "답변:\n",
      "업스테이지의 Solar 모델은 Llama2를 파인튜닝하여 개발된 Solar-0-70b 모델입니다. 이 모델은 한국어와 영어를 지원하며 글로벌 LLM 플랫폼 중 하나인 Poe.com에서 서비스되고 있습니다.\n",
      "========================\n",
      "대화를 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "key = \"MY_OPENAI_API_KEY\"\n",
    "llm_model = 'gpt-3.5-turbo'\n",
    "chatbot_mk4 = RAGChainMake(vector_store, key, llm_model)\n",
    "RAG_Coversation(chatbot_mk4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">같은 질문을 일반 chat gpt 혹은 Gemini에 질문해보고 답변을 비교해보고, 왜 RAG이 필요한지 간단히 markdown으로 서술해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "업스테이지 솔라 모델은 한국의 걸그룹 마마무의 솔라가 모델로 활동하는 뷰티 브랜드입니다. 업스테이지는 솔라의 개성과 매력을 반영한 제품들을 선보이고 있으며, 솔라의 이미지와 함께 다양한 메이크업 제품을 출시하고 있습니다. 솔라의 화려하고 독특한 스타일을 따라가는 소비자들에게 인기가 있는 브랜드로, 솔라의 팬들뿐만 아니라 메이크업을 즐기는 많은 사람들에게 사랑받고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "QUESTION = '업스테이지 solar 모델에 대해서 알려주세요.'\n",
    "model_3 = 'gpt-3.5-turbo'\n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "completion = client.chat.completions.create(\n",
    "        model=model_3,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": QUESTION},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "gpt_3 = completion.choices[0].message.content\n",
    "print(gpt_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "업스테이지(Upstage)에서 개발한 Solar 모델은 자연어 처리(NLP)와 관련된 다양한 작업을 수행할 수 있는 인공지능 모델입니다. 이 모델은 대규모 데이터셋을 기반으로 학습되어, 텍스트 생성, 요약, 번역, 질문 응답 등 여러 가지 언어 관련 작업에서 높은 성능을 발휘합니다.\n",
      "\n",
      "Solar 모델은 특히 한국어 처리에 최적화되어 있으며, 한국어의 문법적 특성과 어휘를 잘 이해하고 있습니다. 이를 통해 한국어로 된 다양한 텍스트에 대한 이해와 생성이 가능하며, 사용자에게 보다 자연스러운 대화 경험을 제공합니다.\n",
      "\n",
      "업스테이지는 Solar 모델을 통해 기업이나 개발자들이 AI 기반의 솔루션을 쉽게 구축할 수 있도록 지원하고 있으며, 다양한 API와 도구를 제공하여 사용자가 자신의 필요에 맞게 모델을 활용할 수 있도록 하고 있습니다.\n",
      "\n",
      "더 구체적인 기능이나 사용 사례에 대해 알고 싶으시면 말씀해 주세요!\n"
     ]
    }
   ],
   "source": [
    "model_4 = 'gpt-4o-mini'\n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "completion = client.chat.completions.create(\n",
    "        model=model_4,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": QUESTION},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "gpt_4 = completion.choices[0].message.content\n",
    "print(gpt_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 답변 비교\n",
    "- 질문 : 업스테이지의 solar 모델에 대해 설명해줘.\n",
    "- 확인 사항 : solar 모델은 2023년에 개발된 업스테이지의 최신 모델\n",
    "\n",
    "### GPT-3.5-turbo\n",
    "- 최신 정보를 반영하지 못하고 할루시네이션 발생\n",
    "\n",
    "### GPT-4o-mini\n",
    "- 최신 정보를 반영하여 solar 모델에 대한 정보를 제공\n",
    "- 상세한 정보를 반영하고 있지 않고 대략적인 정보만 제공\n",
    "\n",
    "### RAG + GPT-3.5-turbo\n",
    "- GPT-3.5-turbo에 없는 정보를 RAG 검색을 이용하여 답변\n",
    "- 업스테이지에서 작성한 문서르 참고했기 떄문에 모델명 등의 상세한 정보를 제공\n",
    "\n",
    "## 결론\n",
    "> RAG를 이용하면 찾는 정보가 LLM의 데이터에 없는 경우나 있더라도 상세한 내용을 설명하지 못하는 경우에 보다 자세하고 신뢰할 수 있는 답변을 생성 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 도전 구현 과제\n",
    "1. LangSmith의 Prompt Library를 참고하여 prompt engineering을 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG_Module 패키지 사용법\n",
    "- Clone한 경우 바로 사용가능\n",
    "\n",
    "1. RAG_Module 폴더 다운로드\n",
    "2. RAG_Module 디렉토리로 이동\n",
    "3. `pip install .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load PDF.....\n",
      "Load Complete....!\n",
      "Split docs.....\n",
      "Split Complete....!\n",
      "\n",
      "Authenticate API KEY....\n",
      "Authenticate Complete....!\n",
      "Set up Embedding model....\n",
      "Set up Complete....!\n",
      "Initiate FAISS instance....\n",
      "Return Vector Store....!\n",
      "\n",
      "prompt_1 이/가 Prompts/ 에 저장되었습니다.\n",
      "\n",
      "prompt_2 이/가 Prompts/ 에 저장되었습니다.\n",
      "\n",
      "load Prompt Template....\n",
      "\n",
      "    필요 없는 부분은 공백('')으로 작성\n",
      "    영어 사용, 명령어 사용\n",
      "    리스트, 마크다운 작성 방식으로 성능을 향상 가능\n",
      "\n",
      "    PERSONA : LLM이 수행할 역할 지정\n",
      "    LANG : 답변 생성 언어\n",
      "    TONE : 답변의 어조 설정\n",
      "    PURPOSE : 목적 명시\n",
      "    HOW_WRITE : 답변 방식 예) 개조식\n",
      "    CONDITION : 추가할 조건\n",
      "    REFERENCE : 참조\n",
      "    \n",
      "Making Prompt... \n",
      "\n",
      "\n",
      "    persona : specialist of large language model\n",
      "    language : only in korean\n",
      "    tone : professional\n",
      "    purpose : study large language model\n",
      "    how to write : itemization\n",
      "    condition : \n",
      "\n",
      "    <must obey>\n",
      "    answer is about large language model\n",
      "    answer that you do not know what you do not know\n",
      "    </must obey>\n",
      "\n",
      "    if you canspecify the date standard of the information\n",
      "    if you can identify the original source of the document you referenced, write in APA format\n",
      "    \n",
      "    reference : omit\n",
      "        \n",
      "load Prompt Template....\n",
      "\n",
      "    필요 없는 부분은 공백('')으로 작성\n",
      "    영어 사용, 명령어 사용\n",
      "    리스트, 마크다운 작성 방식으로 성능을 향상 가능\n",
      "\n",
      "    PERSONA : LLM이 수행할 역할 지정\n",
      "    LANG : 답변 생성 언어\n",
      "    TONE : 답변의 어조 설정\n",
      "    PURPOSE : 목적 명시\n",
      "    HOW_WRITE : 답변 방식 예) 개조식\n",
      "    CONDITION : 추가할 조건\n",
      "    REFERENCE : 참조\n",
      "    \n",
      "Making Prompt... \n",
      "\n",
      "\n",
      "    persona : specialist of large language model\n",
      "    language : only in korean\n",
      "    tone : professional\n",
      "    purpose : study large language model\n",
      "    how to write : itemization\n",
      "    condition : \n",
      "\n",
      "    <must obey>\n",
      "    answer is about large language model\n",
      "    answer that you do not know what you do not know\n",
      "    </must obey>\n",
      "\n",
      "    prioritize the context in question\n",
      "    specify if there are any new information\n",
      "\n",
      "    if you can identify the original source of the document you referenced, write in APA format\n",
      "    \n",
      "    reference : omit\n",
      "        \n",
      "prompt_3 이/가 Prompts/ 에 저장되었습니다.\n",
      "\n",
      "\n",
      "prompt_2.txt 로 시작합니다.\n",
      "prompt_2 이/가 Prompts/ 에서 불러와졌습니다.\n",
      "질문: 업스테이지의 solar 모델에 대해 설명해줘.\n",
      "답변: 업스테이지의 Solar-0-70b 모델은 Llama2를 파인튜닝하여 개발된 한국어 LLM입니다. 이 모델은 한국어와 영어를 모두 지원하며, 글로벌 LLM 플랫폼 중 하나인 Poe.com에서 서비스되고 있습니다.\n",
      "결과를 Results/prompt_2_result_24_11191548.json에 저장 중입니다...\n",
      "결과가 Results/prompt_2_result_24_11191548.json에 json 형식으로 저장되었습니다.\n",
      "\n",
      "prompt_2.txt에 대한 결과 저장 완료.\n",
      "\n",
      "prompt_3.txt 로 시작합니다.\n",
      "prompt_3 이/가 Prompts/ 에서 불러와졌습니다.\n",
      "질문: 업스테이지의 solar 모델에 대해 설명해줘.\n",
      "답변: 업스테이지의 Solar 모델에 대한 설명은 다음과 같습니다:\n",
      "\n",
      "1. **모델 개요**:\n",
      "   - Solar-0-70b 모델은 업스테이지에서 개발된 한국어 대규모 언어 모델입니다.\n",
      "   - 이 모델은 Llama2를 기반으로 하여 파인튜닝(fine-tuning) 기법을 통해 제작되었습니다.\n",
      "\n",
      "2. **기능**:\n",
      "   - Solar 모델은 한국어와 영어 모두를 지원하는 다국어 처리 기능을 갖추고 있습니다.\n",
      "   - 자연어 처리(NLP) 작업에서 텍스트 생성, 질문 응답 등 다양한 기능을 수행할 수 있습니다.\n",
      "\n",
      "3. **훈련 데이터**:\n",
      "   - Solar 모델은 한국어 데이터와 공개된 한국어 데이터, 크롤링 데이터를 활용하여 한국어 토큰 비율을 높여서 학습되었습니다.\n",
      "   - 이는 모델의 한국어 성능을 향상시키는데 중요한 요소로 작용합니다.\n",
      "\n",
      "4. **응용 분야**:\n",
      "   - Solar 모델은 고객 서비스, 콘텐츠 생성, 교육 등 다양한 분야에 적용될 수 있습니다.\n",
      "   - 특히, 한국어 사용자에게 적합한 솔루션을 제공하는 데 중점을 두고 있습니다.\n",
      "\n",
      "5. **기타 정보**:\n",
      "   - Solar 모델은 글로벌 LLM 플랫폼 중 하나인 Poe.com에서 서비스되고 있습니다.\n",
      "\n",
      "이 정보는 '초거대 언어모델 연구 동향' 문서의 내용을 기반으로 작성되었습니다. (Document, 2023)\n",
      "결과를 Results/prompt_3_result_24_11191548.json에 저장 중입니다...\n",
      "결과가 Results/prompt_3_result_24_11191548.json에 json 형식으로 저장되었습니다.\n",
      "\n",
      "prompt_3.txt에 대한 결과 저장 완료.\n",
      "\n",
      "prompt_1.txt 로 시작합니다.\n",
      "prompt_1 이/가 Prompts/ 에서 불러와졌습니다.\n",
      "질문: 업스테이지의 solar 모델에 대해 설명해줘.\n",
      "답변: 업스테이지의 Solar-0-70b 모델은 Llama2를 파인튜닝하여 개발된 한국어 LLM입니다. 이 모델은 글로벌 LLM 플랫폼 중 하나인 Poe.com에서 서비스되고 있으며, 한국어와 영어 모두를 지원합니다.\n",
      "결과를 Results/prompt_1_result_24_11191548.json에 저장 중입니다...\n",
      "결과가 Results/prompt_1_result_24_11191548.json에 json 형식으로 저장되었습니다.\n",
      "\n",
      "prompt_1.txt에 대한 결과 저장 완료.\n",
      "모든 결과가 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from RAG_Module.RAG_Params import *\n",
    "from RAG_Module.PDF_Loader import PDFLoader\n",
    "from RAG_Module.VecotorStore_Utils import VectorStoreReturn\n",
    "from RAG_Module.RAG_Chain import *\n",
    "from RAG_Module.Prompt_Engineering import *\n",
    "\n",
    "rag_setting = RAGParams(\n",
    "    KEY=\"MY_OPENAI_API_KEY\",\n",
    "    EBD_MODEL=\"text-embedding-ada-002\",\n",
    "    LLM_MODEL=\"gpt-3.5-turbo\",\n",
    "    PDF_PATH=\"documents/초거대 언어모델 연구 동향.pdf\",\n",
    "    SAVE_PATH=None,\n",
    "    IS_SAFE=True,\n",
    "    CHUNK_SIZE=500,\n",
    "    CHUNK_OVERLAP=100,\n",
    ")\n",
    "\n",
    "prompt_setting = PromptParams(\n",
    "    KEY=\"MY_OPENAI_API_KEY\",\n",
    "    LLM_MODEL=\"gpt-4o-mini\",\n",
    "    PROMPT_PATH=\"Prompts/\",\n",
    "    PROMPT_NAME=None,\n",
    "    PROMPT_EXTENSION=\"txt\",\n",
    "    RESULT_PATH=\"Results/\",\n",
    "    RESULT_EXTENSION=\"txt\",\n",
    ")\n",
    "\n",
    "# 문서 불러오기 및 분할\n",
    "docs = PDFLoader(rag_setting)\n",
    "\n",
    "# 벡터 스토어 생성\n",
    "vector_store = VectorStoreReturn(docs, rag_setting)\n",
    "\n",
    "# 프롬프트 1 작성 및 저장\n",
    "prompt_1 = \"Answer the question using only the following context.\"\n",
    "PromptSave(prompt_1, prompt_setting, PROMPT_NAME='prompt_1')\n",
    "\n",
    "# 프롬프트 2 작성 및 저장\n",
    "prompt_2 = \"\"\"\n",
    "\n",
    "you are specialist of large language model\n",
    "answer question\n",
    "refer to context qiven in question\n",
    "\n",
    "\"\"\"\n",
    "PromptSave(prompt_2, prompt_setting, PROMPT_NAME='prompt_2')\n",
    "\n",
    "# 프롬프트 3 작성 및 저장\n",
    "\n",
    "## shot 기법 사용을 위한 shot 제작용 프롬프트 생성\n",
    "shot_template = TemplateParams(\n",
    "    PERSONA=\"specialist of large language model\",\n",
    "    LANG=\"only in korean\",\n",
    "    TONE=\"professional\",\n",
    "    PERPOSE=\"study large language model\",\n",
    "    HOW_WRITE=\"itemization\",\n",
    "    CONDITION=\"\"\"\n",
    "\n",
    "    <must obey>\n",
    "    answer is about large language model\n",
    "    answer that you do not know what you do not know\n",
    "    </must obey>\n",
    "\n",
    "    if you canspecify the date standard of the information\n",
    "    if you can identify the original source of the document you referenced, write in APA format\n",
    "    \"\"\",\n",
    "    REFERENCE=\"only the latest information\")\n",
    "shot_prompt = PromptTemplate(shot_template)\n",
    "\n",
    "## shot 생성 - 상위 모델인 gpt-4o-mini르 사용, 답변 방식을 참고하도록 유도\n",
    "question = \"gpt-4에 대해서 설명해줘\"\n",
    "shot = LLMSupport(question, prompt_setting, shot_prompt)\n",
    "\n",
    "## 프롬프트 3 작성 및 저장\n",
    "template_setting = TemplateParams(\n",
    "    PERSONA=\"specialist of large language model\",\n",
    "    LANG=\"only in korean\",\n",
    "    TONE=\"professional\",\n",
    "    PERPOSE=\"study large language model\",\n",
    "    HOW_WRITE=\"itemization\",\n",
    "    CONDITION=\"\"\"\n",
    "\n",
    "    <must obey>\n",
    "    answer is about large language model\n",
    "    answer that you do not know what you do not know\n",
    "    </must obey>\n",
    "\n",
    "    prioritize the context in question\n",
    "    specify if there are any new information\n",
    "\n",
    "    if you can identify the original source of the document you referenced, write in APA format\n",
    "    \"\"\",\n",
    "    REFERENCE=f\"\"\"\n",
    "\n",
    "    <answer format sample>\n",
    "    {shot}\n",
    "    </answer format>\n",
    "    \n",
    "    refer to context given in the question\",\n",
    "    \"\"\"\n",
    ")\n",
    "prompt_3 = PromptTemplate(template_setting)\n",
    "PromptSave(prompt_3, prompt_setting, PROMPT_NAME='prompt_3')\n",
    "\n",
    "# 저장된 프롬프트 1, 2, 3 불러오기 및 결과 저장\n",
    "## QUESTION 리스트 형태로 입력\n",
    "QUESTION = [\"업스테이지의 solar 모델에 대해 설명해줘.\",]\n",
    "AutoChain(prompt_setting, vector_store, QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "LLM_RAG/\n",
    "│\n",
    "├── Prompts/\n",
    "│   ├── prompt_1.txt\n",
    "│   ├── prompt_2.txt\n",
    "│   └── prompt_3.txt\n",
    "└── Results/\n",
    "    ├── prompt_1_result_24_11191548.json\n",
    "    ├── prompt_2_result_24_11191548.json\n",
    "    └── prompt_3_result_24_11191548.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과\n",
    "> RAG로 정보를 제공하고 상대적으로 고성능 모델의 답변 방식을 예시로 제공하면<br>\n",
    ">저성능 모델에서도 의미있는 답변을 얻을 수 있음을 확인"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
